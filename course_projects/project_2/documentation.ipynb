{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3788c572",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/fairseq/tree/main/examples/roberta/commonsense_qa#3-evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9f8bf",
   "metadata": {},
   "source": [
    "# NLP FS25 Course Project 2: Commensense Question Answering with Transformers\n",
    "\n",
    "By David Hodel\n",
    "\n",
    "Weighs & Biases Project: https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99bf15",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e3325",
   "metadata": {},
   "source": [
    "In this notebook, I present my solution to the second course project of the FS25 NLP module at HSLU.\n",
    "\n",
    "The task is to compare three Transformer models on the task of commonsense question answering:\n",
    "1) A randomly initialized Transformer\n",
    "2) A pre-trained Transformer (which was not trained / finetuned on CommonsenseQA)\n",
    "3) An LLM (1B+ parameters) of my choice\n",
    "\n",
    "I'll finetune the first two models and do prompt-engineering for the LLM. The goal is to compare the performance of these three models on the task of commonsense question answering.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We use the CommonsenseQA ([Talmor et al., 2019](https://aclanthology.org/N19-1421/)) dataset in this project. The dataset consists of 12,247 questions with 5 choices each, where only one is correct. The questions are designed to require commonsense reasoning to answer correctly.\n",
    "\n",
    "The dataset was created by taking concepts from ConceptNet, a semantic network of commonsense knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df5cfdc",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb5da4",
   "metadata": {},
   "source": [
    "We first import the necessary libraries to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b89a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torcheval.metrics as metrics\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import AutoModelForCausalLM, pipeline\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6fcab7",
   "metadata": {},
   "source": [
    "We set up a fixed random seed to (at least try to) ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c90f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab939e",
   "metadata": {},
   "source": [
    "Since we use Weights & Biases for experiment tracking, we first have to log in to our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2974460",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b4e4b",
   "metadata": {},
   "source": [
    "If we don't want to run a full hyperparameter search, we can set the id of an existing sweep to load the results from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_sweep_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440e992",
   "metadata": {},
   "source": [
    "### Data Splits\n",
    "\n",
    "The data is available on Hugging Face: https://huggingface.co/datasets/tau/commonsense_qa.\n",
    "Since only the train and validation splits have an answer key, we will use our own dataset splits.\n",
    "We perform the splitting as presented in the lecture slides: We separate the last 1\"000 samples from the training set as the validation set and use the original validation set as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e6065",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a2be2",
   "metadata": {},
   "source": [
    "First, we want to take a look at the data to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "  \"train\": train,\n",
    "  \"validation\": valid,\n",
    "  \"test\": test\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94216e0",
   "metadata": {},
   "source": [
    "We ensure that all three splits have the same structure and that the answers are in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f05911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.column_names)\n",
    "assert train.column_names == valid.column_names == test.column_names\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "unique_answers = set([ex[\"answerKey\"] for ex in train] + [ex[\"answerKey\"] for ex in valid] + [ex[\"answerKey\"] for ex in test])\n",
    "print(f\"Unique answer keys: {unique_answers}\")\n",
    "\n",
    "assert len(unique_answers) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a96b9d",
   "metadata": {},
   "source": [
    "We then display a sample question and its answer for each split to get a feeling of the type of questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caae207",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, data in datasets.items():\n",
    "    print(f\"\\n=== {split} Split ===\")\n",
    "    print(f\"Question: {data[0]['question']}\")\n",
    "    for j, choice in enumerate(data[0]['choices']['text']):\n",
    "        print(f\"{chr(65+j)}) {choice}\")  # A, B, C, etc.\n",
    "    print(f\"Correct Answer: {data[0]['answerKey']}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ea778",
   "metadata": {},
   "source": [
    "We plot the distribution of answer key across the three splits to see if there are any imbalances in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9c284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "labels = sorted(list(unique_answers))\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")[0:5]\n",
    "\n",
    "for i, (split, data) in enumerate(datasets.items()):\n",
    "    answer_counts = Counter([ex[\"answerKey\"] for ex in data])\n",
    "    \n",
    "    # Sort by labels to ensure consistent order\n",
    "    counts = [answer_counts[label] for label in labels]\n",
    "    \n",
    "    ax[i].bar(labels, counts, color=colors)\n",
    "    ax[i].set_xlabel(\"Answer Keys\")\n",
    "    ax[i].set_ylabel(\"Absolute Frequency\")\n",
    "    ax[i].set_title(f\"{split.capitalize()} Set ({len(data)} samples)\")\n",
    "    \n",
    "    # Add percentage annotations\n",
    "    total = sum(counts)\n",
    "    for j, count in enumerate(counts):\n",
    "        percentage = count / total * 100\n",
    "        ax[i].annotate(f\"{percentage:.1f}%\", \n",
    "                      xy=(labels[j], count),\n",
    "                      xytext=(0, 3),\n",
    "                      textcoords=\"offset points\",\n",
    "                      ha='center')\n",
    "\n",
    "plt.suptitle(\"Distribution of Answer Keys Across Dataset Splits\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19890263",
   "metadata": {},
   "source": [
    "We see that the distribution is relatively balanced, with a slight preference for answer `B` in the validation and test set.\n",
    "\n",
    "We also plot the distribution of the number of characters in the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 10))\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")[0:3]\n",
    "all_question_lengths = []\n",
    "\n",
    "for i, (split, data) in enumerate(datasets.items()):\n",
    "  question_lengths = [len(ex[\"question\"]) for ex in data]\n",
    "  all_question_lengths.append(question_lengths)\n",
    "  \n",
    "  # Histogram plots (top row)\n",
    "  axes[0, i].hist(question_lengths, bins=30, color=colors[i])\n",
    "  axes[0, i].set_xlabel(\"Question Length (characters)\")\n",
    "  axes[0, i].set_ylabel(\"Absolute Frequency\")\n",
    "  axes[0, i].set_title(f\"{split.capitalize()} Question Length Distribution\")\n",
    "  \n",
    "  # Add statistics as text\n",
    "  axes[0, i].text(0.6, 0.95, \n",
    "      f\"Min: {min(question_lengths)}\\nMax: {max(question_lengths)}\\nMean: {np.mean(question_lengths):.1f}\\nMedian: {np.median(question_lengths)}\",\n",
    "      transform=axes[0, i].transAxes,\n",
    "      bbox=dict(facecolor='white'),\n",
    "      verticalalignment='top')\n",
    "  \n",
    "  # Boxplot (bottom row)\n",
    "  axes[1, i].boxplot(question_lengths, patch_artist=True)\n",
    "  axes[1, i].set_title(f\"{split.capitalize()} Length Boxplot\")\n",
    "  axes[1, i].set_ylabel(\"Characters\")\n",
    "  \n",
    "  # Set the boxplot fill color\n",
    "  for patch in axes[1, i].get_children():\n",
    "    if isinstance(patch, plt.matplotlib.patches.PathPatch):\n",
    "      patch.set_facecolor(colors[i])\n",
    "\n",
    "plt.suptitle(\"Distribution of Question Lengths Across Dataset Splits\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86bd6cb",
   "metadata": {},
   "source": [
    "We see that the questions are relatively short, with most of them having less than 100 characters. The three splits have a similar distribution and similar mean and median values.\n",
    "\n",
    "The longest question has 376 characters which is good managable for both the Transformer model and the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc4dcc",
   "metadata": {},
   "source": [
    "We do the same check for the question choices and see that the distribution is similar to the questions. The longest question choice has 143 characters which is also manageable for both the Transformer model and the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec6786e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 10))\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")[0:3]\n",
    "all_choice_lengths = []\n",
    "\n",
    "for i, (split, data) in enumerate(datasets.items()):\n",
    "  choice_lengths = np.array([[len(choice) for choice in ex[\"choices\"][\"text\"]] for ex in data]).flatten()\n",
    "  all_choice_lengths.append(choice_lengths)\n",
    "  \n",
    "  # Histogram plots (top row)\n",
    "  axes[0, i].hist(choice_lengths, bins=30, color=colors[i])\n",
    "  axes[0, i].set_xlabel(\"Choice Length (characters)\")\n",
    "  axes[0, i].set_ylabel(\"Absolute Frequency\")\n",
    "  axes[0, i].set_title(f\"{split.capitalize()} Choice Length Distribution\")\n",
    "  \n",
    "  # Add statistics as text\n",
    "  axes[0, i].text(0.6, 0.95, \n",
    "      f\"Min: {min(choice_lengths)}\\nMax: {max(choice_lengths)}\\nMean: {np.mean(choice_lengths):.1f}\\nMedian: {np.median(choice_lengths)}\",\n",
    "      transform=axes[0, i].transAxes,\n",
    "      bbox=dict(facecolor='white'),\n",
    "      verticalalignment='top')\n",
    "  \n",
    "  # Boxplot (bottom row)\n",
    "  axes[1, i].boxplot(choice_lengths, patch_artist=True)\n",
    "  axes[1, i].set_title(f\"{split.capitalize()} Length Boxplot\")\n",
    "  axes[1, i].set_ylabel(\"Characters\")\n",
    "  \n",
    "  # Set the boxplot fill color\n",
    "  for patch in axes[1, i].get_children():\n",
    "    if isinstance(patch, plt.matplotlib.patches.PathPatch):\n",
    "      patch.set_facecolor(colors[i])\n",
    "\n",
    "plt.suptitle(\"Distribution of Choice Lengths Across Dataset Splits\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71eeabd",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8b5e7",
   "metadata": {},
   "source": [
    "No real preprocessing is needed for the data.\n",
    "The DistilRoBERTa model supports any english text, including lowercase / uppercase, punctuation, etc.\n",
    "\n",
    "Same goes for the Phi-LLM, which also accepts any english text.\n",
    "\n",
    "We don't lowercase the text as it may contain important information such as `I` or `US` and because both models and its tokenizers are case-sensitive.\n",
    "\n",
    "We also don't remove stopwords or punctuation as they may also contain important information for the model for answering the questions.\n",
    "\n",
    "Stemming or lemmatization is not necessary the word embeddings and the models can handle different forms of words. Even more, it may be counterproductive as it may remove important information from the text such as time or location information.\n",
    "\n",
    "Removal of unknown words is not necessary as fasttext word vectors are built from vectors of substrings of characters contained in it. This allows to build vectors even for misspelled words or concatenation of words.\n",
    "\n",
    "As the longest question only has 376 characters, we don't need to truncate the text. During training we will use padding to ensure all inputs in a batch have the same length.\n",
    "\n",
    "The text in the dataset is already clean and doesn't contain any html tags we would need to remove and / or parse. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfea23",
   "metadata": {},
   "source": [
    "### Preprocessing Decisions\n",
    "\n",
    "The preprocessing strategy is minimal since transformer models are designed to handle raw text efficiently:\n",
    "\n",
    "- **Tokenization**: We use the model-specific tokenizers (DistilRoBERTa and Phi-4) which handle subword tokenization automatically\n",
    "- **Text Normalization**: No normalization is applied as transformers are robust to capitalization and punctuation\n",
    "- **Special Tokens**: The model tokenizers automatically add [CLS] and [SEP] tokens for sequence classification\n",
    "- **Padding**: Dynamic padding is implemented in the collator to match the longest sequence in each batch\n",
    "- **Input Format**: For transformers, we pair each question with each possible answer, creating 5 input sequences per question\n",
    "- **Label Format**: We convert letter answers (A-E) to integer indices (0-4) for training\n",
    "\n",
    "The choice to keep preprocessing minimal leverages the strengths of transformer models which learn contextual representations directly from raw text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698eb47",
   "metadata": {},
   "source": [
    "### Prepare Data for Tansformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilroberta-base\", use_fast=False) # disable fast tokenizer for multi-threaded tokenization (https://stackoverflow.com/a/72926996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886267a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "encoded = roberta_tokenizer.encode(\"Is this working?\", return_tensors=\"pt\")\n",
    "decoded = roberta_tokenizer.decode(encoded[0])\n",
    "\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78583bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def answer_key_to_index(answer_key):\n",
    "  return ord(answer_key) - ord(\"A\")\n",
    "\n",
    "def index_to_answer_key(index):\n",
    "  return chr(index + ord(\"A\"))\n",
    "\n",
    "assert answer_key_to_index(\"A\") == 0\n",
    "assert index_to_answer_key(0) == \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54429c5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CommonsenseQADataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length, debug=False):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.debug = debug\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        question = example[\"question\"]\n",
    "        choices = example[\"choices\"][\"text\"]\n",
    "        \n",
    "        answer_index = answer_key_to_index(example[\"answerKey\"])\n",
    "            \n",
    "        # Tokenize all question-answer pairs but don't pad yet\n",
    "        encodings = []\n",
    "        for choice in choices:\n",
    "            encoding = self.tokenizer(\n",
    "                question,\n",
    "                choice,\n",
    "                truncation=False,\n",
    "                return_tensors=None  # Return lists, not tensors\n",
    "            )\n",
    "\n",
    "            if self.debug:\n",
    "                # Assert that max_length is respected\n",
    "                assert len(encoding[\"input_ids\"]) <= self.max_length, \"Input exceeds max length\"\n",
    "\n",
    "            encodings.append(encoding)\n",
    "            \n",
    "        return encodings, answer_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f9fec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultipleChoiceCollator:\n",
    "    def __init__(self, tokenizer, debug=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.debug = debug\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Unpack the batch - each item is now a tuple of (encodings, label)\n",
    "        encodings_list = [item[0] for item in batch]  # List of lists of encodings\n",
    "        labels = [item[1] for item in batch]  # List of labels\n",
    "        \n",
    "        # Flatten all encodings\n",
    "        flat_encodings = [encoding for encodings in encodings_list for encoding in encodings]\n",
    "        \n",
    "        # Pad to the longest in this batch\n",
    "        padded_encodings = self.tokenizer.pad(\n",
    "            flat_encodings,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        num_choices = 5\n",
    "        batch_size = len(batch)\n",
    "\n",
    "        # Reshape back to [batch_size, num_choices, seq_length]\n",
    "        input_ids = padded_encodings[\"input_ids\"].view(batch_size, num_choices, -1)\n",
    "        attention_mask = padded_encodings[\"attention_mask\"].view(batch_size, num_choices, -1)\n",
    "        \n",
    "        # Convert labels to tensor\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Return a tuple of (input_ids, attention_mask, labels)\n",
    "        return input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de67fd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CommonsenseQADataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, valid_dataset, test_dataset, tokenizer, batch_size=16, max_length=512, num_workers=8, debug=False):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.num_workers = num_workers\n",
    "        self.debug = debug\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        # Create datasets\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_ds = CommonsenseQADataset(self.train_dataset, self.tokenizer, self.max_length, debug=self.debug)\n",
    "            self.val_ds = CommonsenseQADataset(self.valid_dataset, self.tokenizer, self.max_length, debug=self.debug)\n",
    "            \n",
    "            if self.debug:\n",
    "                # Ensure datasets have expected properties\n",
    "                assert len(self.train_ds) == len(self.train_dataset), \"Train dataset length mismatch\"\n",
    "                assert len(self.val_ds) == len(self.valid_dataset), \"Validation dataset length mismatch\"\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_ds = CommonsenseQADataset(self.test_dataset, self.tokenizer, self.max_length, debug=self.debug)\n",
    "\n",
    "            if self.debug:\n",
    "                assert len(self.test_ds) == len(self.test_dataset), \"Test dataset length mismatch\"\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=MultipleChoiceCollator(self.tokenizer, debug=self.debug),\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "        return loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=MultipleChoiceCollator(self.tokenizer, debug=self.debug),\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "        return loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=MultipleChoiceCollator(self.tokenizer, debug=self.debug),\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320589f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initialize DataModule\n",
    "max_input_length = 512 # 514 (as specified in config.json of distillroberta-base model) - 2 (for [CLS] and [SEP]) \n",
    "data_module = CommonsenseQADataModule(train, valid, test, roberta_tokenizer, batch_size=24, max_length=max_input_length, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be92f515",
   "metadata": {},
   "source": [
    "### Prepare Data for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiPromptDataset(Dataset):\n",
    "    def __init__(self, dataset, random_subset_size=1.0):\n",
    "        self.dataset = dataset\n",
    "        self.prompts = []\n",
    "        self.correct_answers = []\n",
    "\n",
    "        self.prompt_template = \"\"\"<|system|>You are a helpful assistant in a multiple-choice question-answering task. Answer the following multiple-choice commonsense reasoning question with just the letter of the correct answer (A, B, C, D, or E). Do not provide any explanations or additional information. Your response must not contain the full answer, only the letter.<|end|>\n",
    "<|user|>Question: What do students do in school?\n",
    "Choices:\n",
    "A They play outside.\n",
    "B They eat lunch.\n",
    "C They go home.\n",
    "D They learn and study.\n",
    "E They sleep.<|end|>\n",
    "<|assistant|>D<|end|>\n",
    "<|user|>Question: If you leave ice out in the sun, what will most likely happen to it?\n",
    "Choices:\n",
    "A It will catch fire\n",
    "B It will melt\n",
    "C It will grow bigger\n",
    "D It will turn into dust\n",
    "E It will start glowing<|end|>\n",
    "<|assistant|>B<|end|>\n",
    "<|user|>Question: If you are hungry, what is the most logical thing to do?\n",
    "Choices:\n",
    "A Take a nap\n",
    "B Go for a swim\n",
    "C Eat some food\n",
    "D Buy new shoes\n",
    "E Read a book<|end|>\n",
    "<|assistant|>C<|end|>\n",
    "<|user|>Question: {question}\n",
    "Choices:\n",
    "A {choice_a}\n",
    "B {choice_b}\n",
    "C {choice_c}\n",
    "D {choice_d}\n",
    "E {choice_e}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "        \n",
    "        self.prepare_data(random_subset_size)\n",
    "    \n",
    "    def prepare_data(self, random_subset_size=1.0):\n",
    "        # If random_subset_size is 1.0, use the entire dataset\n",
    "        if random_subset_size >= 1.0:\n",
    "            subset = self.dataset\n",
    "        else:\n",
    "            # Calculate the number of examples to include\n",
    "            subset_size = max(1, int(len(self.dataset) * random_subset_size))\n",
    "            \n",
    "            # Get random indices without replacement\n",
    "            indices = random.sample(range(len(self.dataset)), subset_size)\n",
    "            \n",
    "            # Create the subset\n",
    "            subset = [self.dataset[i] for i in indices]\n",
    "        \n",
    "        # Process the subset\n",
    "        for example in subset:\n",
    "            question = example[\"question\"]\n",
    "            choices = example[\"choices\"][\"text\"]\n",
    "            correct_answer = answer_key_to_index(example[\"answerKey\"])\n",
    "            prompt = self.create_prompt(question, choices)\n",
    "            \n",
    "            self.prompts.append(prompt)\n",
    "            self.correct_answers.append(correct_answer)\n",
    "    \n",
    "    def create_prompt(self, question, choices):\n",
    "        prompt = self.prompt_template.format(\n",
    "            question=question,\n",
    "            choice_a=choices[0],\n",
    "            choice_b=choices[1],\n",
    "            choice_c=choices[2],\n",
    "            choice_d=choices[3],\n",
    "            choice_e=choices[4]\n",
    "        )\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"prompt\": self.prompts[idx],\n",
    "            \"correct_answer\": self.correct_answers[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aba168",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_prompt_valid = PhiPromptDataset(valid, random_subset_size=0.1) # around 100 samples is usually enough to see the model's performance\n",
    "phi_prompt_test = PhiPromptDataset(test)\n",
    "\n",
    "phi_prompt_valid[0], phi_prompt_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da659517",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55400a42",
   "metadata": {},
   "source": [
    "### Model Architecture Decisions\n",
    "\n",
    "Three distinct approaches are compared in this project:\n",
    "\n",
    "1. **Pre-trained Transformer**:\n",
    "   - DistilRoBERTa was selected for its balance of performance and efficiency\n",
    "   - The 6-layer, 82M parameter model retains 95% of RoBERTa's performance but runs 2x faster\n",
    "   - Added a custom classification head with intermediate layer normalization and dropout for regularization\n",
    "   - Used the CLS token representation for classification following standard practice\n",
    "\n",
    "2. **Randomly Initialized Transformer**:\n",
    "   - Identical architecture to the pre-trained model but without transfer learning\n",
    "   - Provides a baseline to quantify the value of pre-training\n",
    "   - Uses same hyperparameter configuration for fair comparison\n",
    "\n",
    "3. **Large Language Model**:\n",
    "   - Phi-4-mini (3.8B parameters) was selected for its strong commonsense reasoning capabilities\n",
    "   - Uses few-shot prompting with 3 carefully selected examples rather than fine-tuning\n",
    "   - Prompt template designed to elicit consistent single-letter responses\n",
    "\n",
    "All models share the same cross-entropy loss function but with different learning optimization strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9666fad6",
   "metadata": {},
   "source": [
    "### 1) Pre-trained Transformer\n",
    "\n",
    "I decided to use a distilled version of the [RoBERTa base model](https://huggingface.co/FacebookAI/roberta-base) for this task. The model is available on Hugging Face ([distilbert/distilroberta-base](https://huggingface.co/distilbert/distilroberta-base)) and was trained using the same procedure as DistilBERT.\n",
    "\n",
    "The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base). According to Hugging Face, the model runs on average twice as fast as Roberta-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4b315",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "pretrained_distilroberta = AutoModel.from_pretrained(\"distilbert/distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85007a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RobertaMultipleChoiceModel(pl.LightningModule):\n",
    "    def __init__(self, roberta_model, dropout_prob=0.1, learning_rate=1e-5, weight_decay=1e-3, use_layer_norm=True, hidden_size_multiplier=1.0, debug=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['roberta_model'])\n",
    "\n",
    "        self.roberta = roberta_model\n",
    "        self.roberta.train()\n",
    "\n",
    "        hidden_size = int(hidden_size_multiplier * self.roberta.config.hidden_size)\n",
    "        print(f\"Hidden size: {hidden_size}\")\n",
    "        \n",
    "        # Custom classification head\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size) if use_layer_norm else nn.Identity(),\n",
    "            nn.ReLU(), # non-linearity\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_size, 1) # single score per candidate\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_accuracy = metrics.MulticlassAccuracy(num_classes=5)\n",
    "        self.val_accuracy = metrics.MulticlassAccuracy(num_classes=5)\n",
    "        self.test_accuracy = metrics.MulticlassAccuracy(num_classes=5)\n",
    "\n",
    "        self.test_y = []\n",
    "        self.test_y_pred = []\n",
    "\n",
    "        self.debug = debug\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # input_ids and attention_mask have shape: [batch_size, num_choices, seq_length]\n",
    "        this_batch_size, num_choices, seq_length = input_ids.shape\n",
    "\n",
    "        if self.debug:\n",
    "            assert num_choices == 5, \"Number of choices should be 5 for CommonsenseQA\"\n",
    "            assert seq_length <= self.roberta.config.max_position_embeddings, \"Sequence length exceeds model's max position embeddings\"\n",
    "            assert input_ids.shape == attention_mask.shape, \"Input IDs and attention mask should have the same shape\"\n",
    "        \n",
    "        # Reshape to feed through the model\n",
    "        input_ids = input_ids.view(-1, seq_length)  # [batch_size * num_choices, seq_length]\n",
    "        attention_mask = attention_mask.view(-1, seq_length)  # [batch_size * num_choices, seq_length]\n",
    "\n",
    "        if self.debug:\n",
    "            assert input_ids.shape == attention_mask.shape, \"Input IDs and attention mask should have the same shape\"\n",
    "            assert input_ids.shape[0] == attention_mask.shape[0] == this_batch_size * num_choices, \"First dimension should be batch size * num choices\"\n",
    "            assert input_ids.shape[1] == attention_mask.shape[1] == seq_length, \"Second dimension should be sequence length\"\n",
    "        \n",
    "        # Forward pass through base model\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get the first token (<s>) representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [batch_size * num_choices, hidden_size]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        if self.debug:\n",
    "            assert pooled_output.shape == (this_batch_size * num_choices, self.roberta.config.hidden_size), \"Pooled output should have shape [batch_size * num_choices, hidden_size]\"\n",
    "        \n",
    "        # Get logits for each choice\n",
    "        logits = self.classifier(pooled_output)  # [batch_size * num_choices, 1]\n",
    "\n",
    "        if self.debug:\n",
    "            assert logits.shape == (this_batch_size * num_choices, 1), \"Logits should have shape [batch_size * num_choices, 1]\"\n",
    "        \n",
    "        # Reshape logits back to [batch_size, num_choices]\n",
    "        reshaped_logits = logits.view(this_batch_size, num_choices)\n",
    "\n",
    "        if self.debug:\n",
    "            assert reshaped_logits.shape == (this_batch_size, num_choices), \"Reshaped logits should have shape [batch_size, num_choices]\"\n",
    "        \n",
    "        return reshaped_logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.train_accuracy.update(logits, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", self.train_accuracy.compute().item(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.val_accuracy.update(logits, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy.compute().item(), on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # Update metrics\n",
    "        self.test_accuracy.update(logits, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "        self.log(\"test_acc\", self.test_accuracy.compute().item(), on_epoch=True)\n",
    "\n",
    "        self.test_y_pred.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        self.test_y.extend(labels.cpu().numpy())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def reset_test_arrays(self):\n",
    "        self.test_y = []\n",
    "        self.test_y_pred = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Group parameters to apply a lower learning rate to the transformer layers\n",
    "        transformer_lr_multiplier = 0.05\n",
    "        transformer_lr = transformer_lr_multiplier * self.hparams.learning_rate\n",
    "        classifier_lr = self.hparams.learning_rate\n",
    "\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for _, p in self.roberta.named_parameters()],\n",
    "                \"lr\": transformer_lr,\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for _, p in self.classifier.named_parameters()],\n",
    "                \"lr\": classifier_lr,\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
    "        \n",
    "        # Set up learning rate scheduler\n",
    "        total_steps = self.trainer.estimated_stepping_batches\n",
    "        warmup_steps = total_steps // 10\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=[transformer_lr, classifier_lr],  # Specify max_lr for each group\n",
    "            total_steps=total_steps,\n",
    "            pct_start=warmup_steps / total_steps,\n",
    "            div_factor=100,\n",
    "            final_div_factor=1000,\n",
    "            anneal_strategy=\"linear\"\n",
    "        )\n",
    "        \n",
    "        scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [scheduler_config]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e89e8",
   "metadata": {},
   "source": [
    "### 2) Randomly Initialized Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821eccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a randomly initialized model using the same configuration\n",
    "random_config = AutoConfig.from_pretrained(\"distilbert/distilroberta-base\")\n",
    "random_initialized_roberta = AutoModel.from_config(random_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72476b4",
   "metadata": {},
   "source": [
    "### 3) LLM - Phi-4-mini\n",
    "\n",
    "For the LLM approach, I'll use Phi-4-mini-instruct, which is a 3.8 billion parameter model released by Microsoft in February 2025. The `-instruct` version of the model is designed to follow instructions and answer questions in a conversational manner. The model is available on Hugging Face ([microsoft/Phi-4-mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct)).\n",
    "\n",
    "According to Microsoft, the model is \"built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data\" ([source](https://ollama.com/library/phi4-mini)).\n",
    "\n",
    "\n",
    "\n",
    "Unlike the previous models in this notebook, this model won't be fine-tuned but will use prompt engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8531b82",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "phi_model_name = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(phi_model_name, fast=False) # disable fast tokenizer for multi-threaded tokenization (https://stackoverflow.com/a/72926996)\n",
    "\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    phi_model_name, \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42916bbd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PhiPromptEngineering:\n",
    "    def __init__(self, model, tokenizer, debug=False):\n",
    "        self.model = model\n",
    "        self.debug = debug\n",
    "\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False, # we want deterministic output\n",
    "            return_full_text=False # only return the newly generated text\n",
    "        )\n",
    "    \n",
    "    def predict(self, prompt):\n",
    "        full_response = self.pipe(prompt)[0][\"generated_text\"].strip()\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Full response: '{full_response}'\")\n",
    "        \n",
    "        # Extract just the final letter\n",
    "        # Method 1: Get the very last character if it's a valid choice\n",
    "        final_char = full_response[-1]\n",
    "        if final_char in \"ABCDE\":\n",
    "            answer = final_char\n",
    "        # Method 2: More robust - find the last occurrence of A, B, C, D, or E\n",
    "        else:\n",
    "            for char in reversed(full_response):\n",
    "                if char in \"ABCDE\":\n",
    "                    answer = char\n",
    "                    break\n",
    "            else:  # No valid choice found\n",
    "                raise ValueError(f\"No valid answer choice (A-E) found in response: '{full_response}'\")\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Extracted answer: '{answer}'\")\n",
    "        \n",
    "        assert answer in \"ABCDE\", f\"Extracted answer '{answer}' is not a valid choice (A-E)\"\n",
    "        \n",
    "        return answer_key_to_index(answer.upper())\n",
    "\n",
    "    \n",
    "    def evaluate(self, dataset, log_wandb=True):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        accuracy = 0.0\n",
    "\n",
    "        y = []\n",
    "        y_pred = []\n",
    "        \n",
    "        samples_count = len(dataset)\n",
    "        for i in (pbar := trange(samples_count)):\n",
    "            pbar.set_description(f\"Sample {i}/{samples_count}\")\n",
    "\n",
    "            sample = dataset[i]\n",
    "            prompt, correct_answer = sample[\"prompt\"], sample[\"correct_answer\"]\n",
    "            predicted_index = self.predict(prompt)\n",
    "\n",
    "            y.append(correct_answer)\n",
    "            y_pred.append(predicted_index)\n",
    "            \n",
    "            total += 1\n",
    "            if predicted_index == correct_answer:\n",
    "                correct += 1\n",
    "\n",
    "            accuracy = correct / total\n",
    "            pbar.set_postfix({\"accuracy\": accuracy})\n",
    "\n",
    "            if log_wandb:\n",
    "                wandb.log({\n",
    "                    \"accuracy\": accuracy\n",
    "                })\n",
    "\n",
    "        return accuracy, y, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be30355",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5250b5c",
   "metadata": {},
   "source": [
    "### Training Strategy\n",
    "\n",
    "The training approach is designed to find optimal configurations for each model type:\n",
    "\n",
    "- **Hyperparameter Search**: Using Bayesian optimization (via W&B sweep) to efficiently find optimal values for:\n",
    "  - Learning rate (ranging from 10^-6.5 to 10^-4)\n",
    "  - Weight decay (1e-5 to 1e-3)\n",
    "  - Dropout probability (0.1 to 0.4)\n",
    "  - Layer normalization (on/off)\n",
    "  - Hidden layer size (0.5x to 1.5x base model)\n",
    "\n",
    "- **Optimizer Configuration**:\n",
    "  - Using AdamW with separate learning rates for transformer and classifier components\n",
    "  - Lower learning rate (5% of base) for pre-trained transformer layers to avoid catastrophic forgetting\n",
    "  - OneCycleLR scheduler with 10% warmup period for faster convergence\n",
    "\n",
    "- **Regularization Techniques**:\n",
    "  - Dropout applied at multiple points in the network\n",
    "  - Weight decay to prevent overfitting\n",
    "  - Early stopping with 5-epoch patience monitoring validation accuracy\n",
    "\n",
    "- **Checkpointing**:\n",
    "  - Saving best model based on validation accuracy\n",
    "  - Regular checkpoints every epoch for potential recovery\n",
    "\n",
    "This comprehensive training strategy ensures fair comparison between models while maximizing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d87b3",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa605693",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(model, data_module, max_epochs, checkpoints_path, wandb_run_prefix=None, early_stopping_patience=None, debug=False, existing_run=None):\n",
    "  run = existing_run if existing_run else wandb.init(entity=\"dhodel-hslu-nlp\", project=\"hslu-fs25-nlp-qa-transformers\", name=f\"{wandb_run_prefix}-{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\")\n",
    "\n",
    "  seed_everything(SEED, workers=True)\n",
    "\n",
    "  if early_stopping_patience is None:\n",
    "    early_stopping_patience = max_epochs + 1 # disable early stopping\n",
    "\n",
    "  if debug:\n",
    "    max_epochs = 1\n",
    "\n",
    "  data_module.setup(\"fit\")\n",
    "\n",
    "  best_checkpoint_callback = ModelCheckpoint(\n",
    "      dirpath=checkpoints_path,\n",
    "      filename=\"best-{epoch:02d}-{val_acc:.4f}\",\n",
    "      save_top_k=1,\n",
    "      monitor=\"val_acc\",\n",
    "      mode=\"max\"\n",
    "  )\n",
    "\n",
    "  regular_checkpoint_callback = ModelCheckpoint(\n",
    "      dirpath=checkpoints_path,\n",
    "      filename=\"latest-{epoch:02d}\",\n",
    "      save_top_k=1, # only keep the most recent checkpoint\n",
    "      every_n_epochs=1, # save every epoch\n",
    "  )\n",
    "\n",
    "  early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_acc\",\n",
    "    patience=early_stopping_patience,\n",
    "    mode=\"max\",\n",
    "    check_finite=True # check for NaN or inf values\n",
    "  )\n",
    "  \n",
    "  lr_callback = LearningRateMonitor()\n",
    "\n",
    "  wandb_logger = WandbLogger(\n",
    "    experiment=run,\n",
    "    log_model=(not debug)\n",
    "  )\n",
    "\n",
    "  torch.set_float32_matmul_precision('high')\n",
    "  trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"auto\", # Uses GPU if available, otherwise CPU\n",
    "    callbacks=[best_checkpoint_callback, regular_checkpoint_callback, early_stop_callback, lr_callback],\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=10,\n",
    "  )\n",
    "\n",
    "  trainer.fit(model, data_module)\n",
    "\n",
    "  return trainer, best_checkpoint_callback.best_model_path, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84804756",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the sweep configuration for hyperparameter optimization\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'roberta-hyperparam-sweep-{datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")}',\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [10**-6.5, 10**-6, 10**-5.5, 10**-5, 10**-4.5, 10**-4]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [1e-5, 1e-4, 1e-3]\n",
    "        },\n",
    "        'dropout_prob': {\n",
    "            'values': [0.1, 0.2, 0.3, 0.4]\n",
    "        },\n",
    "        'use_layer_norm': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'hidden_size_multiplier': {\n",
    "            'values': [0.5, 1.0, 1.5]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e6fb0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_sweep_trial():\n",
    "    with wandb.init() as run:\n",
    "        run.name = f\"pretrained-roberta-sweep-{wandb.run.sweep_id}-{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        config = wandb.config\n",
    "\n",
    "        # Create a unique checkpoint directory for this sweep run\n",
    "        sweep_checkpoints_path = f\"./checkpoints/sweep/{wandb.run.id}\"\n",
    "        os.makedirs(sweep_checkpoints_path, exist_ok=True)\n",
    "        \n",
    "        # Create the model with hyperparameters from the sweep\n",
    "        model = RobertaMultipleChoiceModel(\n",
    "            roberta_model=pretrained_distilroberta,\n",
    "            dropout_prob=config.dropout_prob,\n",
    "            learning_rate=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            use_layer_norm=config.use_layer_norm,\n",
    "            hidden_size_multiplier=config.hidden_size_multiplier,\n",
    "            debug=False\n",
    "        )\n",
    "        \n",
    "        # Use the existing train function\n",
    "        _, best_checkpoint, _ = train(\n",
    "            model=model,\n",
    "            data_module=data_module,\n",
    "            max_epochs=50,\n",
    "            checkpoints_path=sweep_checkpoints_path,\n",
    "            early_stopping_patience=5,\n",
    "            debug=False,\n",
    "            existing_run=run\n",
    "        )\n",
    "        \n",
    "        # Save the best checkpoint path for later reference\n",
    "        with open(f\"{sweep_checkpoints_path}/best_checkpoint.txt\", \"w\") as f:\n",
    "            f.write(best_checkpoint)\n",
    "\n",
    "        run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sweep\n",
    "if existing_sweep_id:\n",
    "  # no need to create a new sweep, just use the existing one\n",
    "  sweep_id = existing_sweep_id\n",
    "else:\n",
    "  sweep_id = wandb.sweep(sweep_config, entity=\"dhodel-hslu-nlp\", project=\"hslu-fs25-nlp-qa-transforme rs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ae404",
   "metadata": {},
   "outputs": [],
   "source": [
    "if existing_sweep_id:\n",
    "  print(f\"Using existing sweep ID: {existing_sweep_id}\")\n",
    "else:\n",
    "  wandb.agent(sweep_id, function=run_sweep_trial, count=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10332aa8",
   "metadata": {},
   "source": [
    "### Get Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters from the sweep\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers/{sweep_id}\")\n",
    "best_run = sweep.best_run()\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_config = best_run.config\n",
    "print(\"Best hyperparameters:\")\n",
    "print(f\"Learning Rate: {best_config['learning_rate']}\")\n",
    "print(f\"Weight Decay: {best_config['weight_decay']}\")\n",
    "print(f\"Dropout Probability: {best_config['dropout_prob']}\")\n",
    "print(f\"Use Layer Norm: {best_config['use_layer_norm']}\")\n",
    "print(f\"Hidden Size Multiplier: {best_config['hidden_size_multiplier']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b15669",
   "metadata": {},
   "source": [
    "### 1) Pre-trained Transformer with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61bec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models with the best hyperparameters\n",
    "pretrained_roberta_model = RobertaMultipleChoiceModel(\n",
    "    roberta_model=pretrained_distilroberta, \n",
    "    dropout_prob=best_config['dropout_prob'], \n",
    "    learning_rate=best_config['learning_rate'], \n",
    "    weight_decay=best_config['weight_decay'], \n",
    "    use_layer_norm=best_config['use_layer_norm'], \n",
    "    hidden_size_multiplier=best_config['hidden_size_multiplier'],\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b8bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_checkpoints_path = \"./checkpoints/pretrained\"\n",
    "os.makedirs(pretrained_checkpoints_path, exist_ok=True)\n",
    "\n",
    "pretrained_roberta_trainer, pretrained_roberta_best_checkpoint = train(\n",
    "  model=pretrained_roberta_model,\n",
    "  data_module=data_module,\n",
    "  max_epochs=50,\n",
    "  checkpoints_path=pretrained_checkpoints_path,\n",
    "  early_stopping_patience=5,\n",
    "  wandb_run_prefix=\"pretrained-roberta-best-params\",\n",
    "  debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d1e04",
   "metadata": {},
   "source": [
    "### 2) Randomly Initialized Transformer with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the randomly initialized model with the same best hyperparameters\n",
    "random_initialized_roberta_model = RobertaMultipleChoiceModel(\n",
    "    roberta_model=random_initialized_roberta, \n",
    "    dropout_prob=best_config['dropout_prob'], \n",
    "    learning_rate=best_config['learning_rate'], \n",
    "    weight_decay=best_config['weight_decay'], \n",
    "    use_layer_norm=best_config['use_layer_norm'], \n",
    "    hidden_size_multiplier=best_config['hidden_size_multiplier'],\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_initialized_checkpoints_path = \"./checkpoints/random-initialized\"\n",
    "os.makedirs(random_initialized_checkpoints_path, exist_ok=True)\n",
    "\n",
    "randomly_initialized_roberta_trainer, randomly_initialized_roberta_best_checkpoint = train(\n",
    "  model=random_initialized_roberta_model,\n",
    "  data_module=data_module,\n",
    "  max_epochs=50,\n",
    "  checkpoints_path=random_initialized_checkpoints_path,\n",
    "  early_stopping_patience=5,\n",
    "  wandb_run_prefix=\"random-initialized-roberta-best-params\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cfc6d",
   "metadata": {},
   "source": [
    "### 3) Prompt Engineering with Phi LLM\n",
    "\n",
    "For the Phi model, we don't need training since we're using prompt engineering.\n",
    "\n",
    "For trying out different prompt, I run the mode against a subset of the validation set as the trend of the performance emerges quickly.\n",
    "\n",
    "What I've tried:\n",
    "- Give between 3 and 6 example questions and answers\n",
    "- Include easier and harder examples\n",
    "- Add clarity about tie-breaking: `If multiple options seem correct, choose the MOST appropriate or complete answer`\n",
    "- Chain-of-thought prompting\n",
    "- Encourage double-checking: `Before submitting your answer, verify it makes logical sense and is the most appropriate choice for the question.`\n",
    "\n",
    "In conclusion, the simple prompt with just 2 examples to tune it to the task seems to work best.\n",
    "All othger additions made only a small difference, mostly negative.\n",
    "The chain-of-thought prompting made the evaluating much slower (factor ~50) due to the increased number of tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786143de",
   "metadata": {},
   "source": [
    "### Prompt Engineering Strategy\n",
    "\n",
    "For the Phi-4-mini LLM, a prompt engineering approach was chosen over fine-tuning due to the model's inherent capabilities. The prompt design process involved:\n",
    "\n",
    "- **Few-shot examples**: Providing 3 carefully selected examples that demonstrate the task format and expected answer style\n",
    "- **Task framing**: Clear instructions about returning only a single letter answer (A-E)\n",
    "- **Template structure**: Using the model's native chat format with system, user, and assistant roles\n",
    "- **Prompt iteration**: Testing different prompt variations on a validation subset (10% of validation data)\n",
    "\n",
    "The final prompt template uses a straightforward approach after experiments showed that:\n",
    "- Chain-of-thought reasoning dramatically increased inference time without accuracy improvements\n",
    "- Additional explanation requirements often led to inconsistent formatting\n",
    "- Simple few-shot examples with clear instructions yielded the best performance/speed trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6932bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_prompt_engineering = PhiPromptEngineering(\n",
    "    model=phi_model,\n",
    "    tokenizer=phi_tokenizer,\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    entity=\"dhodel-hslu-nlp\",\n",
    "    project=\"hslu-fs25-nlp-qa-transformers\",\n",
    "    name=f\"phi-prompt-engineering-{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\",\n",
    "    reinit=True,\n",
    "    config={\n",
    "        \"model_name\": phi_model_name,\n",
    "        \"prompt_template\": phi_prompt_valid.prompt_template,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_accuracy, y, y_pred = phi_prompt_engineering.evaluate(phi_prompt_valid, log_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33388dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9f722e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2843239",
   "metadata": {},
   "source": [
    "### Evaluation Methodology\n",
    "\n",
    "A comprehensive evaluation framework was implemented to compare model performance:\n",
    "\n",
    "- **Primary Metric**: Accuracy (proportion of correctly answered questions)\n",
    "- **Secondary Analysis**: Confusion matrices to identify systematic error patterns\n",
    "- **Comprehensive Testing**: Evaluation on a held-out test set (1,221 examples) not used during training\n",
    "- **Visualization**: Comparative bar charts and confusion matrices for intuitive performance comparison\n",
    "\n",
    "The evaluation is designed to not only identify which model performs best numerically, but also to understand the qualitative differences in how models approach the task. This helps assess the value of pre-training, fine-tuning, and prompt engineering for commonsense reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_roberta_trainer.logger = False\n",
    "randomly_initialized_roberta_trainer.logger = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45104383",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_roberta_model.reset_test_arrays()\n",
    "pretrained_test_results = pretrained_roberta_trainer.test(pretrained_roberta_model, datamodule=data_module, ckpt_path=pretrained_roberta_best_checkpoint)\n",
    "\n",
    "random_initialized_roberta_model.reset_test_arrays()\n",
    "random_test_results = randomly_initialized_roberta_trainer.test(random_initialized_roberta_model, datamodule=data_module, ckpt_path=randomly_initialized_roberta_best_checkpoint)\n",
    "\n",
    "pretrained_test_labels = pretrained_roberta_model.test_y\n",
    "pretrained_test_preds = pretrained_roberta_model.test_y_pred\n",
    "\n",
    "random_test_labels = random_initialized_roberta_model.test_y\n",
    "random_test_preds = random_initialized_roberta_model.test_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_accuracy, phi_test_labels, phi_test_preds = phi_prompt_engineering.evaluate(phi_prompt_test, log_wandb=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b7174",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "The confusion matrices reveal distinct error patterns across the three models:\n",
    "\n",
    "- **Pre-trained RoBERTa**:\n",
    "  - Shows a diagonal-dominant pattern indicating strong performance across all answer categories\n",
    "  - Slight tendency to misclassify answer A as B and E\n",
    "  - Most balanced performance across all answer options\n",
    "\n",
    "- **Randomly Initialized RoBERTa**:\n",
    "  - Strong bias toward answering B regardless of the true answer\n",
    "  - Particularly poor performance on answer options C and E\n",
    "  - Shows signs of learning answer distribution statistics rather than content\n",
    "\n",
    "- **Phi-4-mini LLM**:\n",
    "  - Strong overall performance with unique error patterns\n",
    "  - Greater difficulty with answer option E compared to other options\n",
    "  - Less biased toward the most common answer (B) than the randomly initialized model\n",
    "\n",
    "These patterns suggest that pre-training helps the model develop more balanced reasoning abilities across different answer types, while the randomly initialized model falls back to distribution-based guessing. The LLM's unique error pattern suggests it reasons differently than the fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bb94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {i: chr(65 + i) for i in range(5)}  # 0->A, 1->B, etc.\n",
    "label_names = list(label_mapping.values())\n",
    "\n",
    "pretrained_cm = confusion_matrix(pretrained_test_labels, pretrained_test_preds)\n",
    "random_cm = confusion_matrix(random_test_labels, random_test_preds)\n",
    "phi_cm = confusion_matrix(phi_test_labels, phi_test_preds)\n",
    "\n",
    "# Determine the global min and max values for consistent scaling\n",
    "global_vmin = min(pretrained_cm.min(), random_cm.min(), phi_cm.min())\n",
    "global_vmax = max(pretrained_cm.max(), random_cm.max(), phi_cm.max())\n",
    "\n",
    "# Create a figure with three subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "# Plot confusion matrices\n",
    "sns.heatmap(pretrained_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=label_names, yticklabels=label_names, \n",
    "            ax=ax1, vmin=global_vmin, vmax=global_vmax)\n",
    "ax1.set_title(\"Pretrained RoBERTa Model Confusion Matrix\", fontsize=14)\n",
    "ax1.set_xlabel(\"Predicted Choice\", fontsize=12)\n",
    "ax1.set_ylabel(\"True Choice\", fontsize=12)\n",
    "\n",
    "sns.heatmap(random_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=label_names, yticklabels=label_names, \n",
    "            ax=ax2, vmin=global_vmin, vmax=global_vmax)\n",
    "ax2.set_title(\"Random Initialized RoBERTa Model Confusion Matrix\", fontsize=14)\n",
    "ax2.set_xlabel(\"Predicted Choice\", fontsize=12)\n",
    "ax2.set_ylabel(\"True Choice\", fontsize=12)\n",
    "\n",
    "sns.heatmap(phi_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=label_names, yticklabels=label_names, \n",
    "            ax=ax3, vmin=global_vmin, vmax=global_vmax)\n",
    "ax3.set_title(\"Phi-4-mini Confusion Matrix\", fontsize=14)\n",
    "ax3.set_xlabel(\"Predicted Choice\", fontsize=12)\n",
    "ax3.set_ylabel(\"True Choice\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphics/confusion_matrices_all_models.svg\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Create a bar chart to compare model performance\n",
    "model_names = ['Pretrained RoBERTa', 'Random Initialized RoBERTa', 'Phi-4-mini']\n",
    "accuracies = [\n",
    "    pretrained_test_results[0]['test_acc'],\n",
    "    random_test_results[0]['test_acc'],\n",
    "    phi_accuracy\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Performance Comparison', fontsize=14)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "# Add the accuracy values on top of the bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., \n",
    "             bar.get_height() + 0.01, \n",
    "             f'{acc:.4f}', \n",
    "             ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphics/model_comparison.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c92bec",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb5b30",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "\n",
    "The comparison between our three models - pre-trained RoBERTa, randomly initialized RoBERTa, and Phi-4-mini LLM (with prompt engineering) - reveals several interesting patterns in their performance on the CommonsenseQA task.\n",
    "\n",
    "1. **Pre-trained RoBERTa**: The model leverages transfer learning from its pre-training phase, giving it a strong foundation for understanding language patterns and semantics before fine-tuning on our specific task.\n",
    "\n",
    "2. **Randomly Initialized RoBERTa**: Starting from scratch, this model had to learn language patterns solely from our training data, which is much more challenging given the limited size of the dataset compared to typical pre-training datasets.\n",
    "\n",
    "3. **Phi-4-mini LLM (Prompt Engineering)**: This approach uses a much larger model (7B parameters) without any task-specific fine-tuning, relying instead on prompt engineering to elicit the desired behavior.\n",
    "\n",
    "The confusion matrices reveal each model's specific strengths and weaknesses in predicting different answer choices. The bar chart provides a clear comparison of overall accuracy between the three approaches.\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- **Effect of Pre-training**: The significant performance gap between the pre-trained and randomly initialized models demonstrates the value of transfer learning, especially for tasks with limited labeled data.\n",
    "\n",
    "- **LLM with Prompt Engineering**: Phi's performance shows how effectively large language models can be adapted to specific tasks without fine-tuning, using only careful prompt design.\n",
    "\n",
    "- **Error Patterns**: The confusion matrices show different patterns of errors across models, suggesting they may be making different types of mistakes despite being evaluated on the same task.\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "This comparison illustrates the trade-offs between different approaches to transformer-based question answering:\n",
    "\n",
    "- Pre-trained + fine-tuned models offer strong performance with reasonable computational requirements\n",
    "- Randomly initialized models struggle without transfer learning benefits\n",
    "- Large LLMs with prompt engineering can achieve competitive results without task-specific training, but at higher computational cost\n",
    "\n",
    "The results highlight how different transformer-based approaches can be selected based on available resources, performance requirements, and deployment constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c79be1",
   "metadata": {},
   "source": [
    "## Tools Used\n",
    "\n",
    "- Visual Studio Code as IDE\n",
    "- Jupyter Notebook for interactive development\n",
    "- Python 3.9.21\n",
    "- GitHub for version control\n",
    "- Weights & Biases for experiment tracking and hyperparameter optimization\n",
    "- Claude 3.7 Sonnet for troubleshooting, finding bugs and discussing ideas\n",
    "- Github Copilot Chat for troubleshooting and finding bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba60afd",
   "metadata": {},
   "source": [
    "### Development Environment\n",
    "\n",
    "This project was developed using the following tools and infrastructure:\n",
    "\n",
    "- **Development Environment**: Visual Studio Code with Jupyter Notebook integration for interactive code development and visualization\n",
    "- **Version Control**: GitHub for code versioning and collaboration\n",
    "- **Experiment Tracking**: Weights & Biases (wandb) for hyperparameter optimization, experiment logging, and visualization\n",
    "- **Compute Resources**: Local NVIDIA GPU for model training and evaluation\n",
    "- **Libraries**: PyTorch Lightning for training abstractions, Hugging Face Transformers for model implementations\n",
    "- **AI Assistance**: Claude 3.7 Sonnet for code review and error analysis, GitHub Copilot for code suggestions\n",
    "- **Visualization**: Matplotlib and Seaborn for creating publication-quality visualizations\n",
    "\n",
    "The modular architecture and experiment tracking framework enable reproducible research and easy extension to new models or datasets."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "nlp-real",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
