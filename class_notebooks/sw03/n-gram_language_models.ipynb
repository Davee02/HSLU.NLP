{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46cbd05",
   "metadata": {},
   "source": [
    "# N-Gram Language Models\n",
    "In this exercise, we will use n-gram language models to predict the probability of text, and generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a1c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e74f9a",
   "metadata": {},
   "source": [
    "First, we load Jane Austen's Emma from NLTK's gutenberg corpus that we also used in a previous exercise. Tokenize and lowercase this text such that we have a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8dd1c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter']\n",
      "191855\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "raw_text = gutenberg.raw('austen-emma.txt')\n",
    "\n",
    "words = [word.lower() for word in nltk.word_tokenize(raw_text)]\n",
    "print(words[:10])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60adf6a1",
   "metadata": {},
   "source": [
    "Write an n-gram language model class that takes the word list and a parameter `n` as inputs, where `n` is a positive integer larger than 1 that determines the `n` of the n-gram LM. The LM should build a dictionary of n-gram counts from the word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9635e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, words, n):\n",
    "        assert n > 1, \"n needs to be a positive integer > 1\"\n",
    "        assert n <= len(words), \"n can't be larger than the number of words\"\n",
    "\n",
    "        self.n = n\n",
    "        self.ngrams = defaultdict(int) # use defaultdict to avoid having to always check if a key is in the dictionary\n",
    "\n",
    "        for i in range(len(words) - n + 1): # i is the index of the first word in the ngram\n",
    "            ngram = tuple(words[i:i+self.n])\n",
    "            self.ngrams[ngram] += 1\n",
    "\n",
    "            ngram_minus_one = ngram[:-1]\n",
    "            self.ngrams[ngram_minus_one] += 1\n",
    "        # also need to add the last n-1 words\n",
    "        ngram_minus_one = ngram[1:]\n",
    "        self.ngrams[ngram_minus_one] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2d523",
   "metadata": {},
   "source": [
    "Now we \"train\" the n-gram LM by building the n-gram counts of the Emma novel. Use a low `n` (i.e. 2 or 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b49ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramLanguageModel(words, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bf596",
   "metadata": {},
   "source": [
    "Let's add a method `log_probability` to the n-gram LM class that computes the probability of an input string. Since multiplying many probabilities (<= 1) results in very small numbers that can underflow, we sum the log probabilities instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1cfa205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6931471805599453\n",
      "-3.295836866004329\n",
      "0\n",
      "-2.0402208285265546\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def log_probability(self, input_string):\n",
    "    \"\"\" Returns the log-probability of the input string.\"\"\"\n",
    "    # example: [the, cat, sat, on, the, mat]\n",
    "    # p(example) = p(sat | the cat) * p(on | cat sat) * p(the | sat on) * p(mat | on the)\n",
    "    assert len(input_string) > 0, \"input_string must be a non-empty string\"\n",
    "\n",
    "    # 1st step: apply the same preprocessing to the input string as we did to the training data\n",
    "    words = [word.lower() for word in nltk.word_tokenize(input_string)]\n",
    "\n",
    "    # 2nd step: calculate the log-probability\n",
    "    log_probability = 0\n",
    "    for i in range(len(words) - self.n + 1):\n",
    "        ngram = tuple(words[i:i+self.n])\n",
    "        ngram_minus_one = ngram[:-1]\n",
    "\n",
    "        ngram_count = self.ngrams[ngram]\n",
    "        ngram_minus_one_count = self.ngrams[ngram_minus_one]\n",
    "\n",
    "        if ngram_minus_one_count == 0 or ngram_count == 0:\n",
    "            log_probability += 0\n",
    "            continue\n",
    "\n",
    "        log_probability += math.log(ngram_count / ngram_minus_one_count)\n",
    "\n",
    "    return log_probability\n",
    "\n",
    "NGramLanguageModel.log_probability = log_probability\n",
    "\n",
    "print(model.log_probability(\"How are you\"))\n",
    "print(model.log_probability(\"How are you in this longer sentence\"))\n",
    "print(model.log_probability(\"He she weird\"))\n",
    "print(model.log_probability(\"There is a house in New Orleans.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf386295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.03703703703703704\n",
      "1.0\n",
      "0.13\n"
     ]
    }
   ],
   "source": [
    "def inverse_log_probability(log_probability):\n",
    "    return math.e ** log_probability\n",
    "\n",
    "print(inverse_log_probability(model.log_probability(\"How are you\")))\n",
    "print(inverse_log_probability(model.log_probability(\"How are you in this longer sentence\")))\n",
    "print(inverse_log_probability(model.log_probability(\"He she weird\")))\n",
    "print(inverse_log_probability(model.log_probability(\"There is a house in New Orleans.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e7469",
   "metadata": {},
   "source": [
    "Shorter texts will have higher log probability than longer texts, so we need to normalize it by the number of words in the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0338f4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.19245008972987526\n",
      "1.0\n",
      "-2.0402208285265546\n"
     ]
    }
   ],
   "source": [
    "def log_probability_normalized(self, input_string):\n",
    "    \"\"\" Returns the log-probability of the input string.\"\"\"\n",
    "    # example: [the, cat, sat, on, the, mat]\n",
    "    # p(example) = p(sat | the cat) * p(on | cat sat) * p(the | sat on) * p(mat | on the)\n",
    "    assert len(input_string) > 0, \"input_string must be a non-empty string\"\n",
    "\n",
    "    # 1st step: apply the same preprocessing to the input string as we did to the training data\n",
    "    words = [word.lower() for word in nltk.word_tokenize(input_string)]\n",
    "\n",
    "    # 2nd step: calculate the log-probability\n",
    "    normalizing_count = 0\n",
    "    log_probability = 0\n",
    "    for i in range(len(words) - self.n + 1):\n",
    "        ngram = tuple(words[i:i+self.n])\n",
    "        ngram_minus_one = ngram[:-1]\n",
    "\n",
    "        ngram_count = self.ngrams[ngram]\n",
    "        ngram_minus_one_count = self.ngrams[ngram_minus_one]\n",
    "\n",
    "        if ngram_minus_one_count == 0 or ngram_count == 0:\n",
    "            log_probability += 0\n",
    "            continue\n",
    "\n",
    "        log_probability += math.log(ngram_count / ngram_minus_one_count)\n",
    "        normalizing_count += 1 # only increase this if we really have an existing ngram\n",
    "\n",
    "    return 0 if log_probability == 0 else log_probability / normalizing_count\n",
    "\n",
    "NGramLanguageModel.log_probability = log_probability_normalized\n",
    "\n",
    "print(inverse_log_probability(model.log_probability(\"How are you\")))\n",
    "print(inverse_log_probability(model.log_probability(\"How are you in this longer sentence\")))\n",
    "print(inverse_log_probability(model.log_probability(\"He she weird\")))\n",
    "print(model.log_probability(\"There is a house in New Orleans.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e2054",
   "metadata": {},
   "source": [
    "Lets predict the probabilities of two novels under our trained model: Jane Austen's *Sense and Sensibility* (`austen-sense.txt`) and Shakespeare's *Hamlet* (`shakespeare-hamlet.txt`).\n",
    "- What do you expect will happen?\n",
    "- What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4dc2cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.405643228181647\n",
      "-2.7892762868035446\n"
     ]
    }
   ],
   "source": [
    "austen_sense_and_sensibility = gutenberg.raw(\"austen-sense.txt\")\n",
    "shakespeare_hamlet = gutenberg.raw(\"shakespeare-hamlet.txt\")\n",
    "\n",
    "print(model.log_probability(austen_sense_and_sensibility))\n",
    "print(model.log_probability(shakespeare_hamlet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002ddb4",
   "metadata": {},
   "source": [
    "How many n-grams are known in each input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adcafb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10113\n",
      "8219\n"
     ]
    }
   ],
   "source": [
    "# keep track of the known ngrams in the log_probability function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2adf0",
   "metadata": {},
   "source": [
    "Let's add a method `generate` that takes the start of a sentence (\"prompt\") and a number of words to generate, then continues our prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, prompt, num_words=10):\n",
    "    \"\"\" Continues a text starting with `prompt` for the `num_words` next words. \"\"\"\n",
    "    # 1) predict probability of next word\n",
    "    # 2) take word with highest proba\n",
    "    \n",
    "\n",
    "NGramLanguageModel.generate = generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd049682",
   "metadata": {},
   "source": [
    "Play around with a few different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1d951f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.generate(\"While she was sleeping, he was\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
