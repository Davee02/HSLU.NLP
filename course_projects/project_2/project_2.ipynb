{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af323b7",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/fairseq/tree/main/examples/roberta/commonsense_qa#3-evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567d0bf",
   "metadata": {},
   "source": [
    "# NLP FS25 Course Project 2: Commensense Question Answering with Transformers\n",
    "\n",
    "By David Hodel\n",
    "\n",
    "Weighs & Biases Project: https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c56ea5",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b7997",
   "metadata": {},
   "source": [
    "In this notebook, I present my solution to the second course project of the FS25 NLP module at HSLU.\n",
    "\n",
    "The task is to compare three Transformer models on the task of commonsense question answering:\n",
    "1) A randomly initialized Transformer\n",
    "2) A pre-trained Transformer (which was not trained / finetuned on CommonsenseQA)\n",
    "3) An LLM (1B+ parameters) of my choice\n",
    "\n",
    "I'll finetune the first two models and do prompt-engineering for the LLM. The goal is to compare the performance of these three models on the task of commonsense question answering.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We use the CommonsenseQA ([Talmor et al., 2019](https://aclanthology.org/N19-1421/)) dataset in this project. The dataset consists of 12,247 questions with 5 choices each, where only one is correct. The questions are designed to require commonsense reasoning to answer correctly.\n",
    "\n",
    "The dataset was created by taking concepts from ConceptNet, a semantic network of commonsense knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c69ea96",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2408d7d4",
   "metadata": {},
   "source": [
    "We first import the necessary libraries to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c7bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torcheval.metrics as metrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import AutoModelForCausalLM, pipeline\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0621ceb",
   "metadata": {},
   "source": [
    "We set up a fixed random seed to (at least try to) ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2fd8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33279f3",
   "metadata": {},
   "source": [
    "Since we use Weights & Biases for experiment tracking, we first have to log in to our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f625a",
   "metadata": {},
   "source": [
    "### Data Splits\n",
    "\n",
    "The data is available on Hugging Face: https://huggingface.co/datasets/tau/commonsense_qa.\n",
    "Since only the train and validation splits have an answer key, we will use our own dataset splits.\n",
    "We perform the splitting as presented in the lecture slides. We separate the last 1\"000 samples from the training set as the validation set and use the original validation set as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeec9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7e157",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396d1d0f",
   "metadata": {},
   "source": [
    "First, we want to take a look at the data to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1178244",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "  \"train\": train,\n",
    "  \"validation\": valid,\n",
    "  \"test\": test\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74e175",
   "metadata": {},
   "source": [
    "We ensure that all three splits have the same structure and that the answers are in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b54622",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.column_names)\n",
    "assert train.column_names == valid.column_names == test.column_names\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "unique_answers = set([ex[\"answerKey\"] for ex in train] + [ex[\"answerKey\"] for ex in valid] + [ex[\"answerKey\"] for ex in test])\n",
    "print(f\"Unique answer keys: {unique_answers}\")\n",
    "\n",
    "assert len(unique_answers) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30fb378",
   "metadata": {},
   "source": [
    "We then display a sample question and its answer for each split to get a feeling of the type of questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, data in datasets.items():\n",
    "    print(f\"\\n=== {split} Split ===\")\n",
    "    print(f\"Question: {data[0]['question']}\")\n",
    "    for j, choice in enumerate(data[0]['choices']['text']):\n",
    "        print(f\"{chr(65+j)}) {choice}\")  # A, B, C, etc.\n",
    "    print(f\"Correct Answer: {data[0]['answerKey']}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d46f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "labels = sorted(list(unique_answers))\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")[0:5]\n",
    "\n",
    "for i, (split, data) in enumerate(datasets.items()):\n",
    "    answer_counts = Counter([ex[\"answerKey\"] for ex in data])\n",
    "    \n",
    "    # Sort by labels to ensure consistent order\n",
    "    counts = [answer_counts[label] for label in labels]\n",
    "    \n",
    "    ax[i].bar(labels, counts, color=colors)\n",
    "    ax[i].set_xlabel(\"Answer Keys\")\n",
    "    ax[i].set_ylabel(\"Absolute Frequency\")\n",
    "    ax[i].set_title(f\"{split.capitalize()} Set ({len(data)} samples)\")\n",
    "    \n",
    "    # Add percentage annotations\n",
    "    total = sum(counts)\n",
    "    for j, count in enumerate(counts):\n",
    "        percentage = count / total * 100\n",
    "        ax[i].annotate(f\"{percentage:.1f}%\", \n",
    "                      xy=(labels[j], count),\n",
    "                      xytext=(0, 3),\n",
    "                      textcoords=\"offset points\",\n",
    "                      ha='center')\n",
    "\n",
    "plt.suptitle(\"Distribution of Answer Keys Across Dataset Splits\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b521da4",
   "metadata": {},
   "source": [
    "We see that the distribution is relatively balanced, with a slight preference for answer `B` in the validation and test set.\n",
    "\n",
    "We also plot the distribution of the number of characters in the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 10))\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")[0:3]\n",
    "all_question_lengths = []\n",
    "\n",
    "for i, (split, data) in enumerate(datasets.items()):\n",
    "  question_lengths = [len(ex[\"question\"]) for ex in data]\n",
    "  all_question_lengths.append(question_lengths)\n",
    "  \n",
    "  # Histogram plots (top row)\n",
    "  axes[0, i].hist(question_lengths, bins=30, color=colors[i])\n",
    "  axes[0, i].set_xlabel(\"Question Length (characters)\")\n",
    "  axes[0, i].set_ylabel(\"Absolute Frequency\")\n",
    "  axes[0, i].set_title(f\"{split.capitalize()} Question Length Distribution\")\n",
    "  \n",
    "  # Add statistics as text\n",
    "  axes[0, i].text(0.6, 0.95, \n",
    "      f\"Min: {min(question_lengths)}\\nMax: {max(question_lengths)}\\nMean: {np.mean(question_lengths):.1f}\\nMedian: {np.median(question_lengths)}\",\n",
    "      transform=axes[0, i].transAxes,\n",
    "      bbox=dict(facecolor='white'),\n",
    "      verticalalignment='top')\n",
    "  \n",
    "  # Boxplot (bottom row)\n",
    "  axes[1, i].boxplot(question_lengths, patch_artist=True)\n",
    "  axes[1, i].set_title(f\"{split.capitalize()} Length Boxplot\")\n",
    "  axes[1, i].set_ylabel(\"Characters\")\n",
    "  \n",
    "  # Set the boxplot fill color\n",
    "  for patch in axes[1, i].get_children():\n",
    "    if isinstance(patch, plt.matplotlib.patches.PathPatch):\n",
    "      patch.set_facecolor(colors[i])\n",
    "\n",
    "plt.suptitle(\"Distribution of Question Lengths Across Dataset Splits\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eebd3c",
   "metadata": {},
   "source": [
    "We see that the questions are relatively short, with most of them having less than 100 characters. The three splits have a similar distribution and similar mean and median values.\n",
    "\n",
    "The longest question has 376 characters which is good managable for a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8dab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 10))\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")[0:3]\n",
    "all_choice_lengths = []\n",
    "\n",
    "for i, (split, data) in enumerate(datasets.items()):\n",
    "  choice_lengths = np.array([[len(choice) for choice in ex[\"choices\"][\"text\"]] for ex in data]).flatten()\n",
    "  all_choice_lengths.append(choice_lengths)\n",
    "  \n",
    "  # Histogram plots (top row)\n",
    "  axes[0, i].hist(choice_lengths, bins=30, color=colors[i])\n",
    "  axes[0, i].set_xlabel(\"Choice Length (characters)\")\n",
    "  axes[0, i].set_ylabel(\"Absolute Frequency\")\n",
    "  axes[0, i].set_title(f\"{split.capitalize()} Choice Length Distribution\")\n",
    "  \n",
    "  # Add statistics as text\n",
    "  axes[0, i].text(0.6, 0.95, \n",
    "      f\"Min: {min(choice_lengths)}\\nMax: {max(choice_lengths)}\\nMean: {np.mean(choice_lengths):.1f}\\nMedian: {np.median(choice_lengths)}\",\n",
    "      transform=axes[0, i].transAxes,\n",
    "      bbox=dict(facecolor='white'),\n",
    "      verticalalignment='top')\n",
    "  \n",
    "  # Boxplot (bottom row)\n",
    "  axes[1, i].boxplot(choice_lengths, patch_artist=True)\n",
    "  axes[1, i].set_title(f\"{split.capitalize()} Length Boxplot\")\n",
    "  axes[1, i].set_ylabel(\"Characters\")\n",
    "  \n",
    "  # Set the boxplot fill color\n",
    "  for patch in axes[1, i].get_children():\n",
    "    if isinstance(patch, plt.matplotlib.patches.PathPatch):\n",
    "      patch.set_facecolor(colors[i])\n",
    "\n",
    "plt.suptitle(\"Distribution of Choice Lengths Across Dataset Splits\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ae313",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b7e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilroberta-base\", use_fast=False) # disable fast tokenizer for multi-threaded tokenization (https://stackoverflow.com/a/72926996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fc7f8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "encoded = roberta_tokenizer.encode(\"Is this working?\", return_tensors=\"pt\")\n",
    "decoded = roberta_tokenizer.decode(encoded[0])\n",
    "\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f20736",
   "metadata": {},
   "source": [
    "### Prepare Data for Tansformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9797807e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def answer_key_to_index(answer_key):\n",
    "  return ord(answer_key) - ord(\"A\")\n",
    "\n",
    "def index_to_answer_key(index):\n",
    "  return chr(index + ord(\"A\"))\n",
    "\n",
    "assert answer_key_to_index(\"A\") == 0\n",
    "assert index_to_answer_key(0) == \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b71577",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CommonsenseQADataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length, debug=False):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.debug = debug\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        question = example[\"question\"]\n",
    "        choices = example[\"choices\"][\"text\"]\n",
    "        \n",
    "        answer_index = answer_key_to_index(example[\"answerKey\"])\n",
    "            \n",
    "        # Tokenize all question-answer pairs but don't pad yet\n",
    "        encodings = []\n",
    "        for choice in choices:\n",
    "            encoding = self.tokenizer(\n",
    "                question,\n",
    "                choice,\n",
    "                truncation=False,\n",
    "                return_tensors=None  # Return lists, not tensors\n",
    "            )\n",
    "\n",
    "            if self.debug:\n",
    "                # Assert that max_length is respected\n",
    "                assert len(encoding[\"input_ids\"]) <= self.max_length, \"Input exceeds max length\"\n",
    "\n",
    "            encodings.append(encoding)\n",
    "            \n",
    "        return encodings, answer_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c599218",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultipleChoiceCollator:\n",
    "    def __init__(self, tokenizer, debug=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.debug = debug\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Unpack the batch - each item is now a tuple of (encodings, label)\n",
    "        encodings_list = [item[0] for item in batch]  # List of lists of encodings\n",
    "        labels = [item[1] for item in batch]  # List of labels\n",
    "        \n",
    "        # Flatten all encodings\n",
    "        flat_encodings = [encoding for encodings in encodings_list for encoding in encodings]\n",
    "        \n",
    "        # Pad to the longest in this batch\n",
    "        padded_encodings = self.tokenizer.pad(\n",
    "            flat_encodings,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        num_choices = 5\n",
    "        batch_size = len(batch)\n",
    "\n",
    "        # Reshape back to [batch_size, num_choices, seq_length]\n",
    "        input_ids = padded_encodings[\"input_ids\"].view(batch_size, num_choices, -1)\n",
    "        attention_mask = padded_encodings[\"attention_mask\"].view(batch_size, num_choices, -1)\n",
    "        \n",
    "        # Convert labels to tensor\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Return a tuple of (input_ids, attention_mask, labels)\n",
    "        return input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6df0d13c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CommonsenseQADataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, valid_dataset, test_dataset, tokenizer, batch_size=16, max_length=512, num_workers=8, debug=False):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.num_workers = num_workers\n",
    "        self.debug = debug\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        # Create datasets\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_ds = CommonsenseQADataset(self.train_dataset, self.tokenizer, self.max_length, debug=self.debug)\n",
    "            self.val_ds = CommonsenseQADataset(self.valid_dataset, self.tokenizer, self.max_length, debug=self.debug)\n",
    "            \n",
    "            if self.debug:\n",
    "                # Ensure datasets have expected properties\n",
    "                assert len(self.train_ds) == len(self.train_dataset), \"Train dataset length mismatch\"\n",
    "                assert len(self.val_ds) == len(self.valid_dataset), \"Validation dataset length mismatch\"\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_ds = CommonsenseQADataset(self.test_dataset, self.tokenizer, self.max_length, debug=self.debug)\n",
    "\n",
    "            if self.debug:\n",
    "                assert len(self.test_ds) == len(self.test_dataset), \"Test dataset length mismatch\"\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=MultipleChoiceCollator(self.tokenizer, debug=self.debug),\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "        return loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=MultipleChoiceCollator(self.tokenizer, debug=self.debug),\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "        return loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=MultipleChoiceCollator(self.tokenizer, debug=self.debug),\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43a54d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataModule\n",
    "max_input_length = 512 # 514 (as specified in config.json of distillroberta-base model) - 2 (for [CLS] and [SEP]) \n",
    "data_module = CommonsenseQADataModule(train, valid, test, roberta_tokenizer, batch_size=24, max_length=max_input_length, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accbf091",
   "metadata": {},
   "source": [
    "### Prepare Data for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33dbe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiPromptDataset(Dataset):\n",
    "    def __init__(self, dataset, random_subset_size=1.0):\n",
    "        self.dataset = dataset\n",
    "        self.prompts = []\n",
    "        self.correct_answers = []\n",
    "\n",
    "        self.prompt_template = \"\"\"<|system|>You are a helpful assistant in a multiple-choice question-answering task. Answer the following multiple-choice commonsense reasoning question with just the letter of the correct answer (A, B, C, D, or E). Do not provide any explanations or additional information. Your response must not contain the full answer, only the letter.<|end|>\n",
    "<|user|>Question: What do students do in school?\n",
    "Choices:\n",
    "A They play outside.\n",
    "B They eat lunch.\n",
    "C They go home.\n",
    "D They learn and study.\n",
    "E They sleep.<|end|>\n",
    "<|assistant|>D<|end|>\n",
    "<|user|>Question: If you leave ice out in the sun, what will most likely happen to it?\n",
    "Choices:\n",
    "A It will catch fire\n",
    "B It will melt\n",
    "C It will grow bigger\n",
    "D It will turn into dust\n",
    "E It will start glowing<|end|>\n",
    "<|assistant|>B<|end|>\n",
    "<|user|>Question: If you are hungry, what is the most logical thing to do?\n",
    "Choices:\n",
    "A Take a nap\n",
    "B Go for a swim\n",
    "C Eat some food\n",
    "D Buy new shoes\n",
    "E Read a book<|end|>\n",
    "<|assistant|>C<|end|>\n",
    "<|user|>Question: {question}\n",
    "Choices:\n",
    "A {choice_a}\n",
    "B {choice_b}\n",
    "C {choice_c}\n",
    "D {choice_d}\n",
    "E {choice_e}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "        \n",
    "        self.prepare_data(random_subset_size)\n",
    "    \n",
    "    def prepare_data(self, random_subset_size=1.0):\n",
    "        # If random_subset_size is 1.0, use the entire dataset\n",
    "        if random_subset_size >= 1.0:\n",
    "            subset = self.dataset\n",
    "        else:\n",
    "            # Calculate the number of examples to include\n",
    "            subset_size = max(1, int(len(self.dataset) * random_subset_size))\n",
    "            \n",
    "            # Get random indices without replacement\n",
    "            indices = random.sample(range(len(self.dataset)), subset_size)\n",
    "            \n",
    "            # Create the subset\n",
    "            subset = [self.dataset[i] for i in indices]\n",
    "        \n",
    "        # Process the subset\n",
    "        for example in subset:\n",
    "            question = example[\"question\"]\n",
    "            choices = example[\"choices\"][\"text\"]\n",
    "            correct_answer = answer_key_to_index(example[\"answerKey\"])\n",
    "            prompt = self.create_prompt(question, choices)\n",
    "            \n",
    "            self.prompts.append(prompt)\n",
    "            self.correct_answers.append(correct_answer)\n",
    "    \n",
    "    def create_prompt(self, question, choices):\n",
    "        prompt = self.prompt_template.format(\n",
    "            question=question,\n",
    "            choice_a=choices[0],\n",
    "            choice_b=choices[1],\n",
    "            choice_c=choices[2],\n",
    "            choice_d=choices[3],\n",
    "            choice_e=choices[4]\n",
    "        )\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"prompt\": self.prompts[idx],\n",
    "            \"correct_answer\": self.correct_answers[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_prompt_valid = PhiPromptDataset(valid, random_subset_size=0.1) # around 100 samples is usually enough to see the model's performance\n",
    "phi_prompt_test = PhiPromptDataset(test)\n",
    "\n",
    "phi_prompt_valid[0], phi_prompt_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43053dfe",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3289e2f",
   "metadata": {},
   "source": [
    "### 1) Pre-trained Transformer\n",
    "\n",
    "I decided to use a distilled version of the [RoBERTa base model](https://huggingface.co/FacebookAI/roberta-base) for this task. The model is available on Hugging Face ([distilbert/distilroberta-base](https://huggingface.co/distilbert/distilroberta-base)) and was trained using the same procedure as DistilBERT.\n",
    "\n",
    "The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base). According to Hugging Face, the model runs on average twice as fast as Roberta-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d24d9132",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "pretrained_distilroberta = AutoModel.from_pretrained(\"distilbert/distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3158e9e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RobertaMultipleChoiceModel(pl.LightningModule):\n",
    "    def __init__(self, roberta_model, dropout_prob=0.1, learning_rate=1e-5, weight_decay=1e-3, use_layer_norm=True, hidden_size_multiplier=1.0, debug=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['roberta_model'])\n",
    "\n",
    "        self.roberta = roberta_model\n",
    "        self.roberta.train()\n",
    "\n",
    "        hidden_size = int(hidden_size_multiplier * self.roberta.config.hidden_size)\n",
    "        print(f\"Hidden size: {hidden_size}\")\n",
    "        \n",
    "        # Custom classification head\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size) if use_layer_norm else nn.Identity(),\n",
    "            nn.ReLU(), # non-linearity\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_size, 1) # single score per candidate\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_accuracy = metrics.MulticlassAccuracy(num_classes=5)\n",
    "        self.val_accuracy = metrics.MulticlassAccuracy(num_classes=5)\n",
    "        self.test_accuracy = metrics.MulticlassAccuracy(num_classes=5)\n",
    "\n",
    "        self.test_y = []\n",
    "        self.test_y_pred = []\n",
    "\n",
    "        self.debug = debug\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # input_ids and attention_mask have shape: [batch_size, num_choices, seq_length]\n",
    "        this_batch_size, num_choices, seq_length = input_ids.shape\n",
    "\n",
    "        if self.debug:\n",
    "            assert num_choices == 5, \"Number of choices should be 5 for CommonsenseQA\"\n",
    "            assert seq_length <= self.roberta.config.max_position_embeddings, \"Sequence length exceeds model's max position embeddings\"\n",
    "            assert input_ids.shape == attention_mask.shape, \"Input IDs and attention mask should have the same shape\"\n",
    "        \n",
    "        # Reshape to feed through the model\n",
    "        input_ids = input_ids.view(-1, seq_length)  # [batch_size * num_choices, seq_length]\n",
    "        attention_mask = attention_mask.view(-1, seq_length)  # [batch_size * num_choices, seq_length]\n",
    "\n",
    "        if self.debug:\n",
    "            assert input_ids.shape == attention_mask.shape, \"Input IDs and attention mask should have the same shape\"\n",
    "            assert input_ids.shape[0] == attention_mask.shape[0] == this_batch_size * num_choices, \"First dimension should be batch size * num choices\"\n",
    "            assert input_ids.shape[1] == attention_mask.shape[1] == seq_length, \"Second dimension should be sequence length\"\n",
    "        \n",
    "        # Forward pass through base model\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get the first token (<s>) representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [batch_size * num_choices, hidden_size]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        if self.debug:\n",
    "            assert pooled_output.shape == (this_batch_size * num_choices, self.roberta.config.hidden_size), \"Pooled output should have shape [batch_size * num_choices, hidden_size]\"\n",
    "        \n",
    "        # Get logits for each choice\n",
    "        logits = self.classifier(pooled_output)  # [batch_size * num_choices, 1]\n",
    "\n",
    "        if self.debug:\n",
    "            assert logits.shape == (this_batch_size * num_choices, 1), \"Logits should have shape [batch_size * num_choices, 1]\"\n",
    "        \n",
    "        # Reshape logits back to [batch_size, num_choices]\n",
    "        reshaped_logits = logits.view(this_batch_size, num_choices)\n",
    "\n",
    "        if self.debug:\n",
    "            assert reshaped_logits.shape == (this_batch_size, num_choices), \"Reshaped logits should have shape [batch_size, num_choices]\"\n",
    "        \n",
    "        return reshaped_logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.train_accuracy.update(logits, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", self.train_accuracy.compute().item(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.val_accuracy.update(logits, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy.compute().item(), on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # Update metrics\n",
    "        self.test_accuracy.update(logits, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "        self.log(\"test_acc\", self.test_accuracy.compute().item(), on_epoch=True)\n",
    "\n",
    "        self.test_y_pred.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        self.test_y.extend(labels.cpu().numpy())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def reset_test_arrays(self):\n",
    "        self.test_y = []\n",
    "        self.test_y_pred = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Group parameters to apply a lower learning rate to the transformer layers\n",
    "        transformer_lr_multiplier = 0.1\n",
    "        transformer_lr = transformer_lr_multiplier * self.hparams.learning_rate\n",
    "        classifier_lr = self.hparams.learning_rate\n",
    "\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for _, p in self.roberta.named_parameters()],\n",
    "                \"lr\": transformer_lr,\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for _, p in self.classifier.named_parameters()],\n",
    "                \"lr\": classifier_lr,\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
    "        \n",
    "        # Set up learning rate scheduler\n",
    "        total_steps = self.trainer.estimated_stepping_batches\n",
    "        warmup_steps = total_steps // 10\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=[transformer_lr, classifier_lr],  # Specify max_lr for each group\n",
    "            total_steps=total_steps,\n",
    "            pct_start=warmup_steps / total_steps,\n",
    "            div_factor=100,\n",
    "            final_div_factor=1000,\n",
    "            anneal_strategy=\"linear\"\n",
    "        )\n",
    "        \n",
    "        scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [scheduler_config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2125ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_use_layer_norm = True\n",
    "roberta_hidden_size_multiplier = 1.0\n",
    "\n",
    "pretrained_roberta_model = RobertaMultipleChoiceModel(pretrained_distilroberta, dropout_prob=0.1, learning_rate=1e-5, weight_decay=1e-3, use_layer_norm=roberta_use_layer_norm, hidden_size_multiplier=roberta_hidden_size_multiplier, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c3607",
   "metadata": {},
   "source": [
    "### 2) Randomly Initialized Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da50f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a randomly initialized model using the same configuration\n",
    "random_config = AutoConfig.from_pretrained(\"distilbert/distilroberta-base\")\n",
    "random_initialized_roberta = AutoModel.from_config(random_config)\n",
    "\n",
    "# Initialize the PyTorch Lightning model with random weights\n",
    "random_initialized_roberta_model = RobertaMultipleChoiceModel(random_initialized_roberta, dropout_prob=0.1, learning_rate=1e-5, weight_decay=1e-3, use_layer_norm=roberta_use_layer_norm, hidden_size_multiplier=roberta_hidden_size_multiplier, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d64367",
   "metadata": {},
   "source": [
    "### 3) LLM - Phi-4-mini\n",
    "\n",
    "For the LLM approach, I'll use Phi-4-mini, which is a 3.8 billion parameter model released by Microsoft.\n",
    "Unlike the previous models, this model won't be fine-tuned but will use prompt engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc1ba11",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "phi_model_name = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(phi_model_name, fast=False) # disable fast tokenizer for multi-threaded tokenization (https://stackoverflow.com/a/72926996)\n",
    "\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    phi_model_name, \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "131082f0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PhiPromptEngineering:\n",
    "    def __init__(self, model, tokenizer, debug=False):\n",
    "        self.model = model\n",
    "        self.debug = debug\n",
    "\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=1, # only need a short response (just the letter)\n",
    "            do_sample=False, # we want deterministic output\n",
    "            return_full_text=False # only return the newly generated text\n",
    "        )\n",
    "    \n",
    "    def predict(self, prompt):\n",
    "        response = self.pipe(prompt)[0][\"generated_text\"].strip()\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Response: '{response}'\")\n",
    "\n",
    "        assert response, \"Response is empty\"\n",
    "        assert len(response) == 1, \"Response should be a single character\"\n",
    "        assert response[0] in \"ABCDE\", \"Response should be one of the choices (A-E)\"\n",
    "\n",
    "        return answer_key_to_index(response[0].upper())\n",
    "\n",
    "    \n",
    "    def evaluate(self, dataset, log_wandb=True):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        accuracy = 0.0\n",
    "\n",
    "        y = []\n",
    "        y_pred = []\n",
    "        \n",
    "        samples_count = len(dataset)\n",
    "        for i in (pbar := trange(samples_count)):\n",
    "            pbar.set_description(f\"Sample {i}/{samples_count}\")\n",
    "\n",
    "            sample = dataset[i]\n",
    "            prompt, correct_answer = sample[\"prompt\"], sample[\"correct_answer\"]\n",
    "            predicted_index = self.predict(prompt)\n",
    "\n",
    "            y.append(correct_answer)\n",
    "            y_pred.append(predicted_index)\n",
    "            \n",
    "            total += 1\n",
    "            if predicted_index == correct_answer:\n",
    "                correct += 1\n",
    "\n",
    "            accuracy = correct / total\n",
    "            pbar.set_postfix({\"accuracy\": accuracy})\n",
    "\n",
    "            if log_wandb:\n",
    "                wandb.log({\n",
    "                    \"accuracy\": accuracy\n",
    "                })\n",
    "\n",
    "        return accuracy, y, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5b2f3",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b596072",
   "metadata": {},
   "source": [
    "### 1) Pre-trained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe191f17",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(model, data_module, max_epochs, checkpoints_path, wandb_run_prefix, early_stopping_patience=None, debug=False):\n",
    "  if early_stopping_patience is None:\n",
    "    early_stopping_patience = max_epochs + 1 # disable early stopping\n",
    "\n",
    "  if debug:\n",
    "    max_epochs = 1\n",
    "\n",
    "  best_checkpoint_callback = ModelCheckpoint(\n",
    "      dirpath=checkpoints_path,\n",
    "      filename=\"best-{epoch:02d}-{val_acc:.4f}\",\n",
    "      save_top_k=1,\n",
    "      monitor=\"val_acc\",\n",
    "      mode=\"max\"\n",
    "  )\n",
    "\n",
    "  regular_checkpoint_callback = ModelCheckpoint(\n",
    "      dirpath=checkpoints_path,\n",
    "      filename=\"latest-{epoch:02d}\",\n",
    "      save_top_k=1, # only keep the most recent checkpoint\n",
    "      every_n_epochs=1, # save every epoch\n",
    "  )\n",
    "\n",
    "  early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_acc\",\n",
    "    patience=early_stopping_patience,\n",
    "    mode=\"max\"\n",
    "  )\n",
    "  \n",
    "  lr_callback = LearningRateMonitor()\n",
    "\n",
    "  wandb_logger = WandbLogger(\n",
    "    entity=\"dhodel-hslu-nlp\",\n",
    "    project=\"hslu-fs25-nlp-qa-transformers\",\n",
    "    name=f\"{wandb_run_prefix}-{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\",\n",
    "    reinit=True,\n",
    "    log_model=(not debug)\n",
    "  )\n",
    "\n",
    "  torch.set_float32_matmul_precision('high')\n",
    "  trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"auto\", # Uses GPU if available, otherwise CPU\n",
    "    callbacks=[best_checkpoint_callback, regular_checkpoint_callback, early_stop_callback, lr_callback],\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=10,\n",
    "  )\n",
    "\n",
    "  trainer.fit(model, data_module)\n",
    "\n",
    "  wandb.finish()\n",
    "\n",
    "  return trainer, best_checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_checkpoints_path = \"./checkpoints/pretrained\"\n",
    "\n",
    "pretrained_roberta_trainer, pretrained_roberta_best_checkpoint = train(\n",
    "  model=pretrained_roberta_model,\n",
    "  data_module=data_module,\n",
    "  max_epochs=50,\n",
    "  checkpoints_path=pretrained_checkpoints_path,\n",
    "  early_stopping_patience=5,\n",
    "  wandb_run_prefix=\"pretrained-roberta\",\n",
    "  debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d5dd8",
   "metadata": {},
   "source": [
    "### 2) Randomly Initialized Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_initialized_checkpoints_path = \"./checkpoints/random-initialized\"\n",
    "\n",
    "randomly_initialized_roberta_trainer, randomly_initialized_roberta_best_checkpoint = train(\n",
    "  model=random_initialized_roberta_model,\n",
    "  data_module=data_module,\n",
    "  max_epochs=50,\n",
    "  checkpoints_path=random_initialized_checkpoints_path,\n",
    "  early_stopping_patience=5,\n",
    "  wandb_run_prefix=\"random-initialized-roberta\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927fdcf",
   "metadata": {},
   "source": [
    "### 3) Prompt Engineering with Phi LLM\n",
    "\n",
    "For the Phi model, we don't need training since we're using prompt engineering.\n",
    "We'll evaluate the model directly on a sample of the test set to save time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e7b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_prompt_engineering = PhiPromptEngineering(\n",
    "    model=phi_model,\n",
    "    tokenizer=phi_tokenizer,\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1928c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    entity=\"dhodel-hslu-nlp\",\n",
    "    project=\"hslu-fs25-nlp-qa-transformers\",\n",
    "    name=f\"phi-prompt-engineering-{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\",\n",
    "    reinit=True,\n",
    "    config={\n",
    "        \"model_name\": phi_model_name,\n",
    "        \"prompt_template\": phi_prompt_valid.prompt_template,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0539e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_accuracy, y, y_pred = phi_prompt_engineering.evaluate(phi_prompt_valid, log_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be748e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42930e67",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1b41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_roberta_trainer.logger = False\n",
    "randomly_initialized_roberta_trainer.logger = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_roberta_model.reset_test_arrays()\n",
    "pretrained_test_results = pretrained_roberta_trainer.test(pretrained_roberta_model, datamodule=data_module, ckpt_path=pretrained_roberta_best_checkpoint)\n",
    "\n",
    "random_initialized_roberta_model.reset_test_arrays()\n",
    "random_test_results = randomly_initialized_roberta_trainer.test(random_initialized_roberta_model, datamodule=data_module, ckpt_path=randomly_initialized_roberta_best_checkpoint)\n",
    "\n",
    "pretrained_test_labels = pretrained_roberta_model.test_y\n",
    "pretrained_test_preds = pretrained_roberta_model.test_y_pred\n",
    "\n",
    "random_test_labels = random_initialized_roberta_model.test_y\n",
    "random_test_preds = random_initialized_roberta_model.test_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a9dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_accuracy, phi_test_labels, phi_test_preds = phi_prompt_engineering.evaluate(phi_prompt_test, log_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d7208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {i: chr(65 + i) for i in range(5)}  # 0->A, 1->B, etc.\n",
    "label_names = list(label_mapping.values())\n",
    "\n",
    "pretrained_cm = confusion_matrix(pretrained_test_labels, pretrained_test_preds)\n",
    "random_cm = confusion_matrix(random_test_labels, random_test_preds)\n",
    "phi_cm = confusion_matrix(phi_test_labels, phi_test_preds)\n",
    "\n",
    "# Determine the global min and max values for consistent scaling\n",
    "global_vmin = min(pretrained_cm.min(), random_cm.min(), phi_cm.min())\n",
    "global_vmax = max(pretrained_cm.max(), random_cm.max(), phi_cm.max())\n",
    "\n",
    "# Create a figure with three subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "# Plot confusion matrices\n",
    "sns.heatmap(pretrained_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=label_names, yticklabels=label_names, \n",
    "            ax=ax1, vmin=global_vmin, vmax=global_vmax)\n",
    "ax1.set_title(\"Pretrained RoBERTa Model Confusion Matrix\", fontsize=14)\n",
    "ax1.set_xlabel(\"Predicted Choice\", fontsize=12)\n",
    "ax1.set_ylabel(\"True Choice\", fontsize=12)\n",
    "\n",
    "sns.heatmap(random_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=label_names, yticklabels=label_names, \n",
    "            ax=ax2, vmin=global_vmin, vmax=global_vmax)\n",
    "ax2.set_title(\"Random Initialized RoBERTa Model Confusion Matrix\", fontsize=14)\n",
    "ax2.set_xlabel(\"Predicted Choice\", fontsize=12)\n",
    "ax2.set_ylabel(\"True Choice\", fontsize=12)\n",
    "\n",
    "sns.heatmap(phi_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=label_names, yticklabels=label_names, \n",
    "            ax=ax3, vmin=global_vmin, vmax=global_vmax)\n",
    "ax3.set_title(\"Phi-4-mini Confusion Matrix\", fontsize=14)\n",
    "ax3.set_xlabel(\"Predicted Choice\", fontsize=12)\n",
    "ax3.set_ylabel(\"True Choice\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphics/confusion_matrices_all_models.svg\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Create a bar chart to compare model performance\n",
    "model_names = ['Pretrained RoBERTa', 'Random Initialized RoBERTa', 'Phi-4-mini']\n",
    "accuracies = [\n",
    "    pretrained_test_results[0]['test_acc'],\n",
    "    random_test_results[0]['test_acc'],\n",
    "    phi_accuracy\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Performance Comparison', fontsize=14)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "# Add the accuracy values on top of the bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., \n",
    "             bar.get_height() + 0.01, \n",
    "             f'{acc:.4f}', \n",
    "             ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphics/model_comparison.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8ced7",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6096b13f",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "\n",
    "The comparison between our three models - pre-trained RoBERTa, randomly initialized RoBERTa, and Phi-4-mini LLM (with prompt engineering) - reveals several interesting patterns in their performance on the CommonsenseQA task.\n",
    "\n",
    "1. **Pre-trained RoBERTa**: The model leverages transfer learning from its pre-training phase, giving it a strong foundation for understanding language patterns and semantics before fine-tuning on our specific task.\n",
    "\n",
    "2. **Randomly Initialized RoBERTa**: Starting from scratch, this model had to learn language patterns solely from our training data, which is much more challenging given the limited size of the dataset compared to typical pre-training datasets.\n",
    "\n",
    "3. **Phi-4-mini LLM (Prompt Engineering)**: This approach uses a much larger model (7B parameters) without any task-specific fine-tuning, relying instead on prompt engineering to elicit the desired behavior.\n",
    "\n",
    "The confusion matrices reveal each model's specific strengths and weaknesses in predicting different answer choices. The bar chart provides a clear comparison of overall accuracy between the three approaches.\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- **Effect of Pre-training**: The significant performance gap between the pre-trained and randomly initialized models demonstrates the value of transfer learning, especially for tasks with limited labeled data.\n",
    "\n",
    "- **LLM with Prompt Engineering**: Phi's performance shows how effectively large language models can be adapted to specific tasks without fine-tuning, using only careful prompt design.\n",
    "\n",
    "- **Error Patterns**: The confusion matrices show different patterns of errors across models, suggesting they may be making different types of mistakes despite being evaluated on the same task.\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "This comparison illustrates the trade-offs between different approaches to transformer-based question answering:\n",
    "\n",
    "- Pre-trained + fine-tuned models offer strong performance with reasonable computational requirements\n",
    "- Randomly initialized models struggle without transfer learning benefits\n",
    "- Large LLMs with prompt engineering can achieve competitive results without task-specific training, but at higher computational cost\n",
    "\n",
    "The results highlight how different transformer-based approaches can be selected based on available resources, performance requirements, and deployment constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe8ec3",
   "metadata": {},
   "source": [
    "## Tools Used\n",
    "\n",
    "- Visual Studio Code as IDE\n",
    "- Jupyter Notebook for interactive development\n",
    "- Python 3.9.21\n",
    "- GitHub for version control\n",
    "- Weights & Biases for experiment tracking and hyperparameter optimization\n",
    "- Claude 3.7 Sonnet for troubleshooting, finding bugs and discussing ideas\n",
    "- Github Copilot Chat for troubleshooting and finding bugs"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "nlp-real",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
