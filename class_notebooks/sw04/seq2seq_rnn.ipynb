{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20cf81d1",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence RNN\n",
    "In this exercise, we implement a sequence-to-sequence RNN (without attention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827d5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6923b",
   "metadata": {},
   "source": [
    "We first define our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b02ad78",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "sequence_length = 5\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfc188",
   "metadata": {},
   "source": [
    "Create a bidirectional [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) with 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f1c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=False, bidirectional=bidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc0342",
   "metadata": {},
   "source": [
    "We create an example input `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89463769",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(sequence_length, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a3a2c",
   "metadata": {},
   "source": [
    "What should the initial hidden and cell state be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaf1dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 20])\n",
      "torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "num_directions = 2 if bidirectional else 1\n",
    "h0 = torch.zeros(num_directions * num_layers, batch_size, hidden_dim)\n",
    "c0 = torch.zeros(num_directions * num_layers, batch_size, hidden_dim)\n",
    "\n",
    "print(h0.shape)\n",
    "print(c0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3373c",
   "metadata": {},
   "source": [
    "Now we run our LSTM. Look at the output. Explain each dimension of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18b7612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape torch.Size([5, 3, 40])\n",
      "hidden shape torch.Size([4, 3, 20])\n",
      "cell state shape torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "output, (hn, cn) = lstm(x, (h0, c0))\n",
    "\n",
    "# output is the final layer's hidden states\n",
    "print(\"output shape\", output.shape) # sequence length x batch size x 2 times the hidden state\n",
    "print(\"hidden shape\", hn.shape)\n",
    "print(\"cell state shape\", cn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d6d2d",
   "metadata": {},
   "source": [
    "All outputs are from the last (2nd) layer of the LSTM. If we want to have access to the hidden states of layer 1 as well, we have to run the `LSTMCell`s ourselves.\n",
    "\n",
    "When we take the above LSTM as the encoder, what is its output that serves as the input to the decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5386b9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 40])\n"
     ]
    }
   ],
   "source": [
    "encoder = lstm\n",
    "\n",
    "# dim of h[2]: 3 x 20\n",
    "# dim of h[3]: 3 x 20\n",
    "# concatenate along hidden dimenstion (=last dimension)\n",
    "encoder_output = torch.cat([hn[2], hn[3]], dim=-1) # concatenated final hidden states of second layer => 3 x 40 shape\n",
    "print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7afab4",
   "metadata": {},
   "source": [
    "Create a decoder LSTM with 2 layers. Why can't it be bidirectional as well? What is the hidden dimension of the decoder LSTM when you want to initialize it with the encoder output?\n",
    "\n",
    "=> Because we don't know the whole input sequence when we start decoding. We generate the output one token at a time, and we need to know the previous token to generate the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "373c7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden_dim = num_directions * hidden_dim\n",
    "decoder = nn.LSTM(input_size=embedding_dim, hidden_size=decoder_hidden_dim, num_layers=num_layers, batch_first=False, bidirectional=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab709dc",
   "metadata": {},
   "source": [
    "Run your decoder LSTM on an example sequence. Condition it with the encoder representation of the sequence. How do we get the correct shape for the initial hidden state?\n",
    "\n",
    "**Hint:** Take a look at [Torch's tensor operations](https://pytorch.org/docs/stable/tensors.html) and compare `Torch.repeat`, `Torch.repeat_interleave` and `Tensor.expand`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56965f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 40])\n"
     ]
    }
   ],
   "source": [
    "output_seq_len = 8\n",
    "y = torch.randn(output_seq_len, batch_size, embedding_dim)\n",
    "h0_dec = encoder_output.unsqueeze(0).expand(2, -1, -1) # 3 x 40 => 2 x 3 x 40\n",
    "c0_dec = torch.zeros(num_layers, batch_size, decoder_hidden_dim)\n",
    "decoder_output, (hn_dec, cn_dec) = decoder(y, (h0_dec, c0_dec))\n",
    "\n",
    "print(decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ac2ab",
   "metadata": {},
   "source": [
    "In most RNNs, the final encoder hidden state is used as the first hidden state of the decoder RNN. In some variants, it has also been concatenated with the hidden state of the previous time step at each decoder time step. In PyTorch's `nn.LSTM` implementation, we cannot easily do that, so we would have to resort to the lower-level `nn.LSTMCell` class again.\n",
    "\n",
    "Put it all together in a seq2seq LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af981a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seqLSTM(nn.Module):\n",
    "    \"\"\" Sequence-to-sequence LSTM. \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, num_encoder_layers, num_decoder_layers, bidirectional):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.num_decoder_layers = num_decoder_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.decoder_hidden_dim = num_directions * hidden_dim\n",
    "\n",
    "        self.encoder = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_encoder_layers, batch_first=False, bidirectional=bidirectional)\n",
    "        self.decoder = nn.LSTM(input_size=embedding_dim, hidden_size=self.decoder_hidden_dim, num_layers=num_decoder_layers, batch_first=False, bidirectional=False)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        assert x.dim() == 3, \"Expected input of shape [sequence length, batch size, embedding dim]\"\n",
    "        batch_size = x.size(1)\n",
    "\n",
    "        # encoder forward\n",
    "        h0_enc = torch.zeros(self.num_directions * self.num_encoder_layers, batch_size, self.hidden_dim)\n",
    "        c0_enc = torch.zeros(self.num_directions * self.num_encoder_layers, batch_size, self.hidden_dim)\n",
    "        encoder_output, (hn_enc, hn_enc) = self.encoder(x, (h0_enc, c0_enc))\n",
    "\n",
    "        # decoder forward\n",
    "        h0_dec = torch.cat([hn_enc[-2], hn_enc[-1]], dim=-1) if bidirectional else hn_enc[-1]\n",
    "        h0_dec = h0_dec.unsqueeze(0).expand(self.num_decoder_layers, -1, -1)\n",
    "        c0_dec = torch.zeros(self.num_decoder_layers, batch_size, self.decoder_hidden_dim)\n",
    "        decoder_output, (hn_dec, cn_dec) = decoder(y, (h0_dec, c0_dec))\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241dd1ad",
   "metadata": {},
   "source": [
    "Test your seq2seq LSTM with an input sequence `x` and a ground truth output sequence `y` that the decoder tries to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74ef14d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_directions = 2 if bidirectional else 1\n",
    "decoder_hidden_dim = num_directions * hidden_dim\n",
    "seq2seq_lstm = Seq2seqLSTM(embedding_dim, hidden_dim, num_layers, num_layers, bidirectional)\n",
    "x = torch.randn(10, 23, embedding_dim)\n",
    "y = torch.randn(9, 23, embedding_dim)\n",
    "outputs = seq2seq_lstm(x, y)\n",
    "assert outputs.dim() == 3 and list(outputs.size()) == [9, 23, decoder_hidden_dim], \"Wrong output shape\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
