{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47f1ba5",
   "metadata": {},
   "source": [
    "# Text Generation with GPT-2\n",
    "\n",
    "In this exercise, we will use a distilled version of GPT-2 to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c78aee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f007da6",
   "metadata": {},
   "source": [
    "Check out `distilgpt2`'s [model description](https://huggingface.co/distilgpt2) on the Hugging Face model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef04b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "model.eval()\n",
    "sentence = 'Yesterday, I dreamed about being an apple on a cruise through Antarctica.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f05a2",
   "metadata": {},
   "source": [
    "First, we encode the `sentence` with the GPT-2 `tokenizer` and then run a forward pass through the GPT-2 `model` to get familiar with its interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bcde8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "odict_keys(['loss', 'logits', 'past_key_values'])\n",
      "tensor(4.6747)\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(sentence, return_tensors='pt')\n",
    "print(encoded_input.keys())\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(**encoded_input, labels=encoded_input['input_ids'])\n",
    "\n",
    "print(outputs.keys())\n",
    "print(outputs['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f30d4",
   "metadata": {},
   "source": [
    "Compute the perplexity for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0659a4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.539802518759615\n"
     ]
    }
   ],
   "source": [
    "perplexity = 2 ** outputs['loss'].item()\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09093a5e",
   "metadata": {},
   "source": [
    "Now we use the transformer library's `.generate` function by passing `input_ids` and otherwise using the default parameters to generate a continuation to our prompt: \"Yesterday, I dreamed about\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c1298b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday, I dreamed about it. I was a little bit scared of the idea of being a kid. I was a little\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Yesterday, I dreamed about\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032cdf2c",
   "metadata": {},
   "source": [
    "Not bad. Increase the `max_length` argument to `generate` from 20 (default) to 50 and see how the story continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "662227c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday, I dreamed about it. I was a little bit scared of the idea of being a kid. I was a little scared of the idea of being a kid. I was a little scared of the idea of being a kid. I was a\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_length=50)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464e1bb",
   "metadata": {},
   "source": [
    "Uh oh. The model gets stuck in a repetitive loop. Let's prevent that by setting `no_repeat_ngram_size` to 3 (trigram blocking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "382f02b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday, I dreamed about it. I was a little bit scared of the idea of being a kid. I had no idea what it was like to be a kid, and I was so scared of it.\n",
      "\n",
      "\n",
      "I was so excited about\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_length=50, no_repeat_ngram_size=3)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ab3b8",
   "metadata": {},
   "source": [
    "What is the default behavior of `.generate`? Print the model's config to see what generation parameters it uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f4a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d775e",
   "metadata": {},
   "source": [
    "Look at the [documentation of GenerationMixin](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin) to see what decoding method is used with these parameters. Scroll down to the parameters of the `generate` function to see what the default values for e.g. `num_beams` is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791e4b1",
   "metadata": {},
   "source": [
    "**Answer:** greedy decoding (because it uses the default arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e0d32",
   "metadata": {},
   "source": [
    "Let's use beam search with 5 beams instead. Check out the documentation again to see what arguments you have to use for beam search decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f84927a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday, I dreamed about it for a long time, and now I’m finally able to do it again.\n",
      "\n",
      "I’ve been working on it for quite a while now, and it’s finally ready to go.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_length=50, num_beams=5, no_repeat_ngram_size=3)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9ae95",
   "metadata": {},
   "source": [
    "Greedy decoding and beam search are deterministic decoding methods. If you want, you can run the previous generations again and see that the output doesn't change.\n",
    "\n",
    "Let's now change to probabilistic decoding to get more diverse texts. Set `do_sample` to True and `num_beams` to 1. Execute your generation multiple times and see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1a31d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday, I dreamed about making this very fast paced movie. I got sick of the plot in a horrible way, and I couldn't figure out that I would create it properly.\n",
      "\n",
      "I have also been trying to do something with the film,\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_length=50, do_sample=True)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411c057",
   "metadata": {},
   "source": [
    "If you run this generation multiple times, you will sometimes see weird outputs. This happens when a low-probability token gets sampled. To avoid this, we limit the options to the top-*k* tokens of the next-token distribution. Set `top_k` to 5 and 50, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "421f093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday, I dreamed about a future in the future. I wanted it to be like the world I am. I was so happy to see what happened, and I wanted my family to be happy and proud of me, but I also wanted my family\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_length=50, do_sample=True, top_k=5)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c61acc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yesterday, I dreamed about getting it done once a while but sadly it wasn't. I just started feeling depressed. I remember reading one of my friends's blogs that said things like, \"My friends are going to go down and eat my mom and\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, pad_token_id=tokenizer.eos_token_id, max_length=50, do_sample=True, top_k=50)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbf39a",
   "metadata": {},
   "source": [
    "Try the same with top-*p* sampling and vary *p*, e.g. use 0.1, 0.8 and 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da2b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-real",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
