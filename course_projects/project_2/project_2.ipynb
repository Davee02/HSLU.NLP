{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30e725c4",
   "metadata": {},
   "source": [
    "https://github.com/facebookresearch/fairseq/tree/main/examples/roberta/commonsense_qa#3-evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a63b4",
   "metadata": {},
   "source": [
    "# NLP FS25 Course Project 2: Commensense Question Answering with Transformers\n",
    "\n",
    "By David Hodel\n",
    "\n",
    "Weighs & Biases Project: https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3108e1d1",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385689f",
   "metadata": {},
   "source": [
    "In this notebook, I present my solution to the second course project of the FS25 NLP module at HSLU.\n",
    "\n",
    "The task is to compare three Transformer models on the task of commonsense question answering:\n",
    "1) A randomly initialized Transformer\n",
    "2) A pre-trained Transformer (which was not trained / finetuned on CommonsenseQA)\n",
    "3) An LLM (1B+ parameters) of my choice\n",
    "\n",
    "I'll finetune the first two models and do prompt-engineering for the LLM. The goal is to compare the performance of these three models on the task of commonsense question answering.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We use the CommonsenseQA ([Talmor et al., 2019](https://aclanthology.org/N19-1421/)) dataset in this project. The dataset consists of 12,247 questions with 5 choices each, where only one is correct. The questions are designed to require commonsense reasoning to answer correctly.\n",
    "\n",
    "The dataset was created by taking concepts from ConceptNet, a semantic network of commonsense knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299e878",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c921fb9",
   "metadata": {},
   "source": [
    "We first import the necessary libraries to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bd0365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import trange\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torcheval.metrics as metrics\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import AutoModelForCausalLM, pipeline\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054a5ca",
   "metadata": {},
   "source": [
    "We set up a fixed random seed to (at least try to) ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8fae60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c10dae",
   "metadata": {},
   "source": [
    "Since we use Weights & Biases for experiment tracking, we first have to log in to our account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f05557aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdavidhodel\u001b[0m (\u001b[33mwaitless-hslu-dspro2-fs25\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_sweep_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce2306",
   "metadata": {},
   "source": [
    "### Data Splits\n",
    "\n",
    "The data is available on Hugging Face: https://huggingface.co/datasets/tau/commonsense_qa.\n",
    "Since only the train and validation splits have an answer key, we will use our own dataset splits.\n",
    "We perform the splitting as presented in the lecture slides. We separate the last 1\"000 samples from the training set as the validation set and use the original validation set as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c669849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8741 1000 1221\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231736b",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3e3c7",
   "metadata": {},
   "source": [
    "First, we want to take a look at the data to understand its structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1401e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "  \"train\": train,\n",
    "  \"validation\": valid,\n",
    "  \"test\": test\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41d6b2",
   "metadata": {},
   "source": [
    "We ensure that all three splits have the same structure and that the answers are in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559abd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.column_names)\n",
    "assert train.column_names == valid.column_names == test.column_names\n",
    "\n",
    "print(train[0])\n",
    "\n",
    "unique_answers = set([ex[\"answerKey\"] for ex in train] + [ex[\"answerKey\"] for ex in valid] + [ex[\"answerKey\"] for ex in test])\n",
    "print(f\"Unique answer keys: {unique_answers}\")\n",
    "\n",
    "assert len(unique_answers) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6068274",
   "metadata": {},
   "source": [
    "We then display a sample question and its answer for each split to get a feeling of the type of questions and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, data in datasets.items():\n",
    "    print(f\"\\n=== {split} Split ===\")\n",
    "    print(f\"Question: {data[0]['question']}\")\n",
    "    for j, choice in enumerate(data[0]['choices']['text']):\n",
    "        print(f\"{chr(65+j)}) {choice}\")  # A, B, C, etc.\n",
    "    print(f\"Correct Answer: {data[0]['answerKey']}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26319ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "labels = sorted(list(unique_answers))\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")[0:5]\n",
    "\n",
    "for i, (split, data) in enumerate(datasets.items()):\n",
    "    answer_counts = Counter([ex[\"answerKey\"] for ex in data])\n",
    "    \n",
    "    # Sort by labels to ensure consistent order\n",
    "    counts = [answer_counts[label] for label in labels]\n",
    "    \n",
    "    ax[i].bar(labels, counts, color=colors)\n",
    "    ax[i].set_xlabel(\"Answer Keys\")\n",
    "    ax[i].set_ylabel(\"Absolute Frequency\")\n",
    "    ax[i].set_title(f\"{split.capitalize()} Set ({len(data)} samples)\")\n",
    "    \n",
    "    # Add percentage annotations\n",
    "    total = sum(counts)\n",
    "    for j, count in enumerate(counts):\n",
    "        percentage = count / total * 100\n",
    "        ax[i].annotate(f\"{percentage:.1f}%\", \n",
    "                      xy=(labels[j], count),\n",
    "                      xytext=(0, 3),\n",
    "                      textcoords=\"offset points\",\n",
    "                      ha='center')\n",
    "\n",
    "plt.suptitle(\"Distribution of Answer Keys Across Dataset Splits\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c9eae",
   "metadata": {},
   "source": [
    "We see that the distribution is relatively balanced, with a slight preference for answer `B` in the validation and test set.\n",
    "\n",
    "We also plot the distribution of the number of characters in the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 10))\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")[0:3]\n",
    "all_question_lengths = []\n",
    "\n",
    "for i, (split, data) in enumerate(datasets.items()):\n",
    "  question_lengths = [len(ex[\"question\"]) for ex in data]\n",
    "  all_question_lengths.append(question_lengths)\n",
    "  \n",
    "  # Histogram plots (top row)\n",
    "  axes[0, i].hist(question_lengths, bins=30, color=colors[i])\n",
    "  axes[0, i].set_xlabel(\"Question Length (characters)\")\n",
    "  axes[0, i].set_ylabel(\"Absolute Frequency\")\n",
    "  axes[0, i].set_title(f\"{split.capitalize()} Question Length Distribution\")\n",
    "  \n",
    "  # Add statistics as text\n",
    "  axes[0, i].text(0.6, 0.95, \n",
    "      f\"Min: {min(question_lengths)}\\nMax: {max(question_lengths)}\\nMean: {np.mean(question_lengths):.1f}\\nMedian: {np.median(question_lengths)}\",\n",
    "      transform=axes[0, i].transAxes,\n",
    "      bbox=dict(facecolor='white'),\n",
    "      verticalalignment='top')\n",
    "  \n",
    "  # Boxplot (bottom row)\n",
    "  axes[1, i].boxplot(question_lengths, patch_artist=True)\n",
    "  axes[1, i].set_title(f\"{split.capitalize()} Length Boxplot\")\n",
    "  axes[1, i].set_ylabel(\"Characters\")\n",
    "  \n",
    "  # Set the boxplot fill color\n",
    "  for patch in axes[1, i].get_children():\n",
    "    if isinstance(patch, plt.matplotlib.patches.PathPatch):\n",
    "      patch.set_facecolor(colors[i])\n",
    "\n",
    "plt.suptitle(\"Distribution of Question Lengths Across Dataset Splits\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88057f29",
   "metadata": {},
   "source": [
    "We see that the questions are relatively short, with most of them having less than 100 characters. The three splits have a similar distribution and similar mean and median values.\n",
    "\n",
    "The longest question has 376 characters which is good managable for a transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 10))\n",
    "\n",
    "colors = sns.color_palette(\"pastel\")[0:3]\n",
    "all_choice_lengths = []\n",
    "\n",
    "for i, (split, data) in enumerate(datasets.items()):\n",
    "  choice_lengths = np.array([[len(choice) for choice in ex[\"choices\"][\"text\"]] for ex in data]).flatten()\n",
    "  all_choice_lengths.append(choice_lengths)\n",
    "  \n",
    "  # Histogram plots (top row)\n",
    "  axes[0, i].hist(choice_lengths, bins=30, color=colors[i])\n",
    "  axes[0, i].set_xlabel(\"Choice Length (characters)\")\n",
    "  axes[0, i].set_ylabel(\"Absolute Frequency\")\n",
    "  axes[0, i].set_title(f\"{split.capitalize()} Choice Length Distribution\")\n",
    "  \n",
    "  # Add statistics as text\n",
    "  axes[0, i].text(0.6, 0.95, \n",
    "      f\"Min: {min(choice_lengths)}\\nMax: {max(choice_lengths)}\\nMean: {np.mean(choice_lengths):.1f}\\nMedian: {np.median(choice_lengths)}\",\n",
    "      transform=axes[0, i].transAxes,\n",
    "      bbox=dict(facecolor='white'),\n",
    "      verticalalignment='top')\n",
    "  \n",
    "  # Boxplot (bottom row)\n",
    "  axes[1, i].boxplot(choice_lengths, patch_artist=True)\n",
    "  axes[1, i].set_title(f\"{split.capitalize()} Length Boxplot\")\n",
    "  axes[1, i].set_ylabel(\"Characters\")\n",
    "  \n",
    "  # Set the boxplot fill color\n",
    "  for patch in axes[1, i].get_children():\n",
    "    if isinstance(patch, plt.matplotlib.patches.PathPatch):\n",
    "      patch.set_facecolor(colors[i])\n",
    "\n",
    "plt.suptitle(\"Distribution of Choice Lengths Across Dataset Splits\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb7bc8",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610e449",
   "metadata": {},
   "source": [
    "No real preprocessing is needed for the data.\n",
    "The DistilRoBERTa model supports any english text, including lowercase / uppercase, punctuation, etc.\n",
    "\n",
    "Same goes for the Phi-LLM, which also accepts any english text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b457a4",
   "metadata": {},
   "source": [
    "### Prepare Data for Tansformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a49bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilroberta-base\", use_fast=False) # disable fast tokenizer for multi-threaded tokenization (https://stackoverflow.com/a/72926996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecdb8ffc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: tensor([[   0, 6209,   42,  447,  116,    2]])\n",
      "Decoded: <s>Is this working?</s>\n"
     ]
    }
   ],
   "source": [
    "encoded = roberta_tokenizer.encode(\"Is this working?\", return_tensors=\"pt\")\n",
    "decoded = roberta_tokenizer.decode(encoded[0])\n",
    "\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83c6e73c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def answer_key_to_index(answer_key):\n",
    "  return ord(answer_key) - ord(\"A\")\n",
    "\n",
    "def index_to_answer_key(index):\n",
    "  return chr(index + ord(\"A\"))\n",
    "\n",
    "assert answer_key_to_index(\"A\") == 0\n",
    "assert index_to_answer_key(0) == \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75b22221",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CommonsenseQADataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length, debug=False):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.debug = debug\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        question = example[\"question\"]\n",
    "        choices = example[\"choices\"][\"text\"]\n",
    "        \n",
    "        answer_index = answer_key_to_index(example[\"answerKey\"])\n",
    "            \n",
    "        # Tokenize all question-answer pairs but don't pad yet\n",
    "        encodings = []\n",
    "        for choice in choices:\n",
    "            encoding = self.tokenizer(\n",
    "                question,\n",
    "                choice,\n",
    "                truncation=False,\n",
    "                return_tensors=None  # Return lists, not tensors\n",
    "            )\n",
    "\n",
    "            if self.debug:\n",
    "                # Assert that max_length is respected\n",
    "                assert len(encoding[\"input_ids\"]) <= self.max_length, \"Input exceeds max length\"\n",
    "\n",
    "            encodings.append(encoding)\n",
    "            \n",
    "        return encodings, answer_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d869efa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MultipleChoiceCollator:\n",
    "    def __init__(self, tokenizer, debug=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.debug = debug\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Unpack the batch - each item is now a tuple of (encodings, label)\n",
    "        encodings_list = [item[0] for item in batch]  # List of lists of encodings\n",
    "        labels = [item[1] for item in batch]  # List of labels\n",
    "        \n",
    "        # Flatten all encodings\n",
    "        flat_encodings = [encoding for encodings in encodings_list for encoding in encodings]\n",
    "        \n",
    "        # Pad to the longest in this batch\n",
    "        padded_encodings = self.tokenizer.pad(\n",
    "            flat_encodings,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        num_choices = 5\n",
    "        batch_size = len(batch)\n",
    "\n",
    "        # Reshape back to [batch_size, num_choices, seq_length]\n",
    "        input_ids = padded_encodings[\"input_ids\"].view(batch_size, num_choices, -1)\n",
    "        attention_mask = padded_encodings[\"attention_mask\"].view(batch_size, num_choices, -1)\n",
    "        \n",
    "        # Convert labels to tensor\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Return a tuple of (input_ids, attention_mask, labels)\n",
    "        return input_ids, attention_mask, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd7704a9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CommonsenseQADataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, valid_dataset, test_dataset, tokenizer, batch_size=16, max_length=512, num_workers=8, debug=False):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.num_workers = num_workers\n",
    "        self.debug = debug\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        # Create datasets\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_ds = CommonsenseQADataset(self.train_dataset, self.tokenizer, self.max_length, debug=self.debug)\n",
    "            self.val_ds = CommonsenseQADataset(self.valid_dataset, self.tokenizer, self.max_length, debug=self.debug)\n",
    "            \n",
    "            if self.debug:\n",
    "                # Ensure datasets have expected properties\n",
    "                assert len(self.train_ds) == len(self.train_dataset), \"Train dataset length mismatch\"\n",
    "                assert len(self.val_ds) == len(self.valid_dataset), \"Validation dataset length mismatch\"\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_ds = CommonsenseQADataset(self.test_dataset, self.tokenizer, self.max_length, debug=self.debug)\n",
    "\n",
    "            if self.debug:\n",
    "                assert len(self.test_ds) == len(self.test_dataset), \"Test dataset length mismatch\"\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=MultipleChoiceCollator(self.tokenizer, debug=self.debug),\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "        return loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=MultipleChoiceCollator(self.tokenizer, debug=self.debug),\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "        return loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=MultipleChoiceCollator(self.tokenizer, debug=self.debug),\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "        return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61c4e965",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Initialize DataModule\n",
    "max_input_length = 512 # 514 (as specified in config.json of distillroberta-base model) - 2 (for [CLS] and [SEP]) \n",
    "data_module = CommonsenseQADataModule(train, valid, test, roberta_tokenizer, batch_size=24, max_length=max_input_length, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c53ed2",
   "metadata": {},
   "source": [
    "### Prepare Data for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef386f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiPromptDataset(Dataset):\n",
    "    def __init__(self, dataset, random_subset_size=1.0):\n",
    "        self.dataset = dataset\n",
    "        self.prompts = []\n",
    "        self.correct_answers = []\n",
    "\n",
    "        self.prompt_template = \"\"\"<|system|>You are a helpful assistant in a multiple-choice question-answering task. Answer the following multiple-choice commonsense reasoning question with just the letter of the correct answer (A, B, C, D, or E). Do not provide any explanations or additional information. Your response must not contain the full answer, only the letter.<|end|>\n",
    "<|user|>Question: What do students do in school?\n",
    "Choices:\n",
    "A They play outside.\n",
    "B They eat lunch.\n",
    "C They go home.\n",
    "D They learn and study.\n",
    "E They sleep.<|end|>\n",
    "<|assistant|>D<|end|>\n",
    "<|user|>Question: If you leave ice out in the sun, what will most likely happen to it?\n",
    "Choices:\n",
    "A It will catch fire\n",
    "B It will melt\n",
    "C It will grow bigger\n",
    "D It will turn into dust\n",
    "E It will start glowing<|end|>\n",
    "<|assistant|>B<|end|>\n",
    "<|user|>Question: If you are hungry, what is the most logical thing to do?\n",
    "Choices:\n",
    "A Take a nap\n",
    "B Go for a swim\n",
    "C Eat some food\n",
    "D Buy new shoes\n",
    "E Read a book<|end|>\n",
    "<|assistant|>C<|end|>\n",
    "<|user|>Question: {question}\n",
    "Choices:\n",
    "A {choice_a}\n",
    "B {choice_b}\n",
    "C {choice_c}\n",
    "D {choice_d}\n",
    "E {choice_e}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "        \n",
    "        self.prepare_data(random_subset_size)\n",
    "    \n",
    "    def prepare_data(self, random_subset_size=1.0):\n",
    "        # If random_subset_size is 1.0, use the entire dataset\n",
    "        if random_subset_size >= 1.0:\n",
    "            subset = self.dataset\n",
    "        else:\n",
    "            # Calculate the number of examples to include\n",
    "            subset_size = max(1, int(len(self.dataset) * random_subset_size))\n",
    "            \n",
    "            # Get random indices without replacement\n",
    "            indices = random.sample(range(len(self.dataset)), subset_size)\n",
    "            \n",
    "            # Create the subset\n",
    "            subset = [self.dataset[i] for i in indices]\n",
    "        \n",
    "        # Process the subset\n",
    "        for example in subset:\n",
    "            question = example[\"question\"]\n",
    "            choices = example[\"choices\"][\"text\"]\n",
    "            correct_answer = answer_key_to_index(example[\"answerKey\"])\n",
    "            prompt = self.create_prompt(question, choices)\n",
    "            \n",
    "            self.prompts.append(prompt)\n",
    "            self.correct_answers.append(correct_answer)\n",
    "    \n",
    "    def create_prompt(self, question, choices):\n",
    "        prompt = self.prompt_template.format(\n",
    "            question=question,\n",
    "            choice_a=choices[0],\n",
    "            choice_b=choices[1],\n",
    "            choice_c=choices[2],\n",
    "            choice_d=choices[3],\n",
    "            choice_e=choices[4]\n",
    "        )\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"prompt\": self.prompts[idx],\n",
    "            \"correct_answer\": self.correct_answers[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de58418",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_prompt_valid = PhiPromptDataset(valid, random_subset_size=0.1) # around 100 samples is usually enough to see the model's performance\n",
    "phi_prompt_test = PhiPromptDataset(test)\n",
    "\n",
    "phi_prompt_valid[0], phi_prompt_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b054e4a",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c007895",
   "metadata": {},
   "source": [
    "### 1) Pre-trained Transformer\n",
    "\n",
    "I decided to use a distilled version of the [RoBERTa base model](https://huggingface.co/FacebookAI/roberta-base) for this task. The model is available on Hugging Face ([distilbert/distilroberta-base](https://huggingface.co/distilbert/distilroberta-base)) and was trained using the same procedure as DistilBERT.\n",
    "\n",
    "The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base). According to Hugging Face, the model runs on average twice as fast as Roberta-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f00725",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "pretrained_distilroberta = AutoModel.from_pretrained(\"distilbert/distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "418b18c9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RobertaMultipleChoiceModel(pl.LightningModule):\n",
    "    def __init__(self, roberta_model, dropout_prob=0.1, learning_rate=1e-5, weight_decay=1e-3, use_layer_norm=True, hidden_size_multiplier=1.0, debug=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['roberta_model'])\n",
    "\n",
    "        self.roberta = roberta_model\n",
    "        self.roberta.train()\n",
    "\n",
    "        hidden_size = int(hidden_size_multiplier * self.roberta.config.hidden_size)\n",
    "        print(f\"Hidden size: {hidden_size}\")\n",
    "        \n",
    "        # Custom classification head\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size) if use_layer_norm else nn.Identity(),\n",
    "            nn.ReLU(), # non-linearity\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_size, 1) # single score per candidate\n",
    "        )\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_accuracy = metrics.MulticlassAccuracy(num_classes=5)\n",
    "        self.val_accuracy = metrics.MulticlassAccuracy(num_classes=5)\n",
    "        self.test_accuracy = metrics.MulticlassAccuracy(num_classes=5)\n",
    "\n",
    "        self.test_y = []\n",
    "        self.test_y_pred = []\n",
    "\n",
    "        self.debug = debug\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # input_ids and attention_mask have shape: [batch_size, num_choices, seq_length]\n",
    "        this_batch_size, num_choices, seq_length = input_ids.shape\n",
    "\n",
    "        if self.debug:\n",
    "            assert num_choices == 5, \"Number of choices should be 5 for CommonsenseQA\"\n",
    "            assert seq_length <= self.roberta.config.max_position_embeddings, \"Sequence length exceeds model's max position embeddings\"\n",
    "            assert input_ids.shape == attention_mask.shape, \"Input IDs and attention mask should have the same shape\"\n",
    "        \n",
    "        # Reshape to feed through the model\n",
    "        input_ids = input_ids.view(-1, seq_length)  # [batch_size * num_choices, seq_length]\n",
    "        attention_mask = attention_mask.view(-1, seq_length)  # [batch_size * num_choices, seq_length]\n",
    "\n",
    "        if self.debug:\n",
    "            assert input_ids.shape == attention_mask.shape, \"Input IDs and attention mask should have the same shape\"\n",
    "            assert input_ids.shape[0] == attention_mask.shape[0] == this_batch_size * num_choices, \"First dimension should be batch size * num choices\"\n",
    "            assert input_ids.shape[1] == attention_mask.shape[1] == seq_length, \"Second dimension should be sequence length\"\n",
    "        \n",
    "        # Forward pass through base model\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get the first token (<s>) representation\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [batch_size * num_choices, hidden_size]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        if self.debug:\n",
    "            assert pooled_output.shape == (this_batch_size * num_choices, self.roberta.config.hidden_size), \"Pooled output should have shape [batch_size * num_choices, hidden_size]\"\n",
    "        \n",
    "        # Get logits for each choice\n",
    "        logits = self.classifier(pooled_output)  # [batch_size * num_choices, 1]\n",
    "\n",
    "        if self.debug:\n",
    "            assert logits.shape == (this_batch_size * num_choices, 1), \"Logits should have shape [batch_size * num_choices, 1]\"\n",
    "        \n",
    "        # Reshape logits back to [batch_size, num_choices]\n",
    "        reshaped_logits = logits.view(this_batch_size, num_choices)\n",
    "\n",
    "        if self.debug:\n",
    "            assert reshaped_logits.shape == (this_batch_size, num_choices), \"Reshaped logits should have shape [batch_size, num_choices]\"\n",
    "        \n",
    "        return reshaped_logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.train_accuracy.update(logits, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", self.train_accuracy.compute().item(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.val_accuracy.update(logits, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy.compute().item(), on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        # Update metrics\n",
    "        self.test_accuracy.update(logits, labels)\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"test_loss\", loss, on_epoch=True)\n",
    "        self.log(\"test_acc\", self.test_accuracy.compute().item(), on_epoch=True)\n",
    "\n",
    "        self.test_y_pred.extend(logits.argmax(dim=1).cpu().numpy())\n",
    "        self.test_y.extend(labels.cpu().numpy())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def reset_test_arrays(self):\n",
    "        self.test_y = []\n",
    "        self.test_y_pred = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Group parameters to apply a lower learning rate to the transformer layers\n",
    "        transformer_lr_multiplier = 0.05\n",
    "        transformer_lr = transformer_lr_multiplier * self.hparams.learning_rate\n",
    "        classifier_lr = self.hparams.learning_rate\n",
    "\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for _, p in self.roberta.named_parameters()],\n",
    "                \"lr\": transformer_lr,\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for _, p in self.classifier.named_parameters()],\n",
    "                \"lr\": classifier_lr,\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
    "        \n",
    "        # Set up learning rate scheduler\n",
    "        total_steps = self.trainer.estimated_stepping_batches\n",
    "        warmup_steps = total_steps // 10\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=[transformer_lr, classifier_lr],  # Specify max_lr for each group\n",
    "            total_steps=total_steps,\n",
    "            pct_start=warmup_steps / total_steps,\n",
    "            div_factor=100,\n",
    "            final_div_factor=1000,\n",
    "            anneal_strategy=\"linear\"\n",
    "        )\n",
    "        \n",
    "        scheduler_config = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "        \n",
    "        return [optimizer], [scheduler_config]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332daa39",
   "metadata": {},
   "source": [
    "### 2) Randomly Initialized Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ec13f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a randomly initialized model using the same configuration\n",
    "random_config = AutoConfig.from_pretrained(\"distilbert/distilroberta-base\")\n",
    "random_initialized_roberta = AutoModel.from_config(random_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125d146",
   "metadata": {},
   "source": [
    "### 3) LLM - Phi-4-mini\n",
    "\n",
    "For the LLM approach, I'll use Phi-4-mini-instruct, which is a 3.8 billion parameter model released by Microsoft in February 2025. The `-instruct` version of the model is designed to follow instructions and answer questions in a conversational manner. The model is available on Hugging Face ([microsoft/Phi-4-mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct)).\n",
    "\n",
    "According to Microsoft, the model is \"built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data\" ([source](https://ollama.com/library/phi4-mini)).\n",
    "\n",
    "\n",
    "\n",
    "Unlike the previous models in this notebook, this model won't be fine-tuned but will use prompt engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaac69b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "phi_model_name = \"microsoft/Phi-4-mini-instruct\"\n",
    "\n",
    "phi_tokenizer = AutoTokenizer.from_pretrained(phi_model_name, fast=False) # disable fast tokenizer for multi-threaded tokenization (https://stackoverflow.com/a/72926996)\n",
    "\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "    phi_model_name, \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d5932",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PhiPromptEngineering:\n",
    "    def __init__(self, model, tokenizer, debug=False):\n",
    "        self.model = model\n",
    "        self.debug = debug\n",
    "\n",
    "        self.pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False, # we want deterministic output\n",
    "            return_full_text=False # only return the newly generated text\n",
    "        )\n",
    "    \n",
    "    def predict(self, prompt):\n",
    "        full_response = self.pipe(prompt)[0][\"generated_text\"].strip()\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Full response: '{full_response}'\")\n",
    "        \n",
    "        # Extract just the final letter\n",
    "        # Method 1: Get the very last character if it's a valid choice\n",
    "        final_char = full_response[-1]\n",
    "        if final_char in \"ABCDE\":\n",
    "            answer = final_char\n",
    "        # Method 2: More robust - find the last occurrence of A, B, C, D, or E\n",
    "        else:\n",
    "            for char in reversed(full_response):\n",
    "                if char in \"ABCDE\":\n",
    "                    answer = char\n",
    "                    break\n",
    "            else:  # No valid choice found\n",
    "                raise ValueError(f\"No valid answer choice (A-E) found in response: '{full_response}'\")\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Extracted answer: '{answer}'\")\n",
    "        \n",
    "        assert answer in \"ABCDE\", f\"Extracted answer '{answer}' is not a valid choice (A-E)\"\n",
    "        \n",
    "        return answer_key_to_index(answer.upper())\n",
    "\n",
    "    \n",
    "    def evaluate(self, dataset, log_wandb=True):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        accuracy = 0.0\n",
    "\n",
    "        y = []\n",
    "        y_pred = []\n",
    "        \n",
    "        samples_count = len(dataset)\n",
    "        for i in (pbar := trange(samples_count)):\n",
    "            pbar.set_description(f\"Sample {i}/{samples_count}\")\n",
    "\n",
    "            sample = dataset[i]\n",
    "            prompt, correct_answer = sample[\"prompt\"], sample[\"correct_answer\"]\n",
    "            predicted_index = self.predict(prompt)\n",
    "\n",
    "            y.append(correct_answer)\n",
    "            y_pred.append(predicted_index)\n",
    "            \n",
    "            total += 1\n",
    "            if predicted_index == correct_answer:\n",
    "                correct += 1\n",
    "\n",
    "            accuracy = correct / total\n",
    "            pbar.set_postfix({\"accuracy\": accuracy})\n",
    "\n",
    "            if log_wandb:\n",
    "                wandb.log({\n",
    "                    \"accuracy\": accuracy\n",
    "                })\n",
    "\n",
    "        return accuracy, y, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2589f8e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568cdcdc",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3db930d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(model, data_module, max_epochs, checkpoints_path, wandb_run_prefix=None, early_stopping_patience=None, debug=False, existing_run=None):\n",
    "  run = existing_run if existing_run else wandb.init(entity=\"dhodel-hslu-nlp\", project=\"hslu-fs25-nlp-qa-transformers\", name=f\"{wandb_run_prefix}-{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\")\n",
    "\n",
    "  seed_everything(SEED, workers=True)\n",
    "\n",
    "  if early_stopping_patience is None:\n",
    "    early_stopping_patience = max_epochs + 1 # disable early stopping\n",
    "\n",
    "  if debug:\n",
    "    max_epochs = 1\n",
    "\n",
    "  data_module.setup(\"fit\")\n",
    "\n",
    "  best_checkpoint_callback = ModelCheckpoint(\n",
    "      dirpath=checkpoints_path,\n",
    "      filename=\"best-{epoch:02d}-{val_acc:.4f}\",\n",
    "      save_top_k=1,\n",
    "      monitor=\"val_acc\",\n",
    "      mode=\"max\"\n",
    "  )\n",
    "\n",
    "  regular_checkpoint_callback = ModelCheckpoint(\n",
    "      dirpath=checkpoints_path,\n",
    "      filename=\"latest-{epoch:02d}\",\n",
    "      save_top_k=1, # only keep the most recent checkpoint\n",
    "      every_n_epochs=1, # save every epoch\n",
    "  )\n",
    "\n",
    "  early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_acc\",\n",
    "    patience=early_stopping_patience,\n",
    "    mode=\"max\",\n",
    "    check_finite=True # check for NaN or inf values\n",
    "  )\n",
    "  \n",
    "  lr_callback = LearningRateMonitor()\n",
    "\n",
    "  wandb_logger = WandbLogger(\n",
    "    experiment=run,\n",
    "    log_model=(not debug)\n",
    "  )\n",
    "\n",
    "  torch.set_float32_matmul_precision('high')\n",
    "  trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"auto\", # Uses GPU if available, otherwise CPU\n",
    "    callbacks=[best_checkpoint_callback, regular_checkpoint_callback, early_stop_callback, lr_callback],\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=10,\n",
    "  )\n",
    "\n",
    "  trainer.fit(model, data_module)\n",
    "\n",
    "  return trainer, best_checkpoint_callback.best_model_path, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acb3ee9d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the sweep configuration for hyperparameter optimization\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'name': f'roberta-hyperparam-sweep-{datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\")}',\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'values': [3e-6, 5e-6, 1e-5, 3e-5, 1e-4, 3e-4]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [1e-5, 1e-4, 1e-3]\n",
    "        },\n",
    "        'dropout_prob': {\n",
    "            'values': [0.1, 0.2, 0.3, 0.4]\n",
    "        },\n",
    "        'use_layer_norm': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'hidden_size_multiplier': {\n",
    "            'values': [0.5, 1.0, 1.5]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a78eada",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_sweep_trial():\n",
    "    with wandb.init() as run:\n",
    "        run.name = f\"pretrained-roberta-sweep-{wandb.run.sweep_id}-{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        config = wandb.config\n",
    "\n",
    "        # Create a unique checkpoint directory for this sweep run\n",
    "        sweep_checkpoints_path = f\"./checkpoints/sweep/{wandb.run.id}\"\n",
    "        os.makedirs(sweep_checkpoints_path, exist_ok=True)\n",
    "        \n",
    "        # Create the model with hyperparameters from the sweep\n",
    "        model = RobertaMultipleChoiceModel(\n",
    "            roberta_model=pretrained_distilroberta,\n",
    "            dropout_prob=config.dropout_prob,\n",
    "            learning_rate=config.learning_rate,\n",
    "            weight_decay=config.weight_decay,\n",
    "            use_layer_norm=config.use_layer_norm,\n",
    "            hidden_size_multiplier=config.hidden_size_multiplier,\n",
    "            debug=False\n",
    "        )\n",
    "        \n",
    "        # Use the existing train function\n",
    "        _, best_checkpoint, _ = train(\n",
    "            model=model,\n",
    "            data_module=data_module,\n",
    "            max_epochs=50,\n",
    "            checkpoints_path=sweep_checkpoints_path,\n",
    "            early_stopping_patience=5,\n",
    "            debug=False,\n",
    "            existing_run=run\n",
    "        )\n",
    "        \n",
    "        # Save the best checkpoint path for later reference\n",
    "        with open(f\"{sweep_checkpoints_path}/best_checkpoint.txt\", \"w\") as f:\n",
    "            f.write(best_checkpoint)\n",
    "\n",
    "        run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc2d7c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: b89yrm6b\n",
      "Sweep URL: https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers/sweeps/b89yrm6b\n"
     ]
    }
   ],
   "source": [
    "# Initialize the sweep\n",
    "if existing_sweep_id:\n",
    "  # no need to create a new sweep, just use the existing one\n",
    "  sweep_id = existing_sweep_id\n",
    "else:\n",
    "  sweep_id = wandb.sweep(sweep_config, entity=\"dhodel-hslu-nlp\", project=\"hslu-fs25-nlp-qa-transforme rs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9846c78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6lbkb493 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_prob: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size_multiplier: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_layer_norm: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'hslu-fs25-nlp-qa-transformers' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'dhodel-hslu-nlp' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/david/git/HSLU.NLP/course_projects/project_2/wandb/run-20250501_123734-6lbkb493</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers/runs/6lbkb493' target=\"_blank\">eager-sweep-3</a></strong> to <a href='https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers/sweeps/b89yrm6b' target=\"_blank\">https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers/sweeps/b89yrm6b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers' target=\"_blank\">https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers/sweeps/b89yrm6b' target=\"_blank\">https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers/sweeps/b89yrm6b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers/runs/6lbkb493' target=\"_blank\">https://wandb.ai/dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers/runs/6lbkb493</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name       | Type             | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | roberta    | RobertaModel     | 82.1 M | train\n",
      "1 | dropout    | Dropout          | 0      | train\n",
      "2 | classifier | Sequential       | 592 K  | train\n",
      "3 | criterion  | CrossEntropyLoss | 0      | train\n",
      "--------------------------------------------------------\n",
      "82.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "82.7 M    Total params\n",
      "330.845   Total estimated model params size (MB)\n",
      "128       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dropout_prob' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_layer_norm' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'hidden_size_multiplier' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c631d0539c841939b2ca489872573b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cd328665ea4c9fbdc9c2005d91ae82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a7e4820098430eb0c7894f3b9ea9cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9321fabf00b24e2694fbb9f253d79722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if existing_sweep_id:\n",
    "  print(f\"Using existing sweep ID: {existing_sweep_id}\")\n",
    "else:\n",
    "  wandb.agent(sweep_id, function=run_sweep_trial, count=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b84873",
   "metadata": {},
   "source": [
    "### Get Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ac53c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Problem finishing run\n",
      "Exception in thread Thread-35:\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_25416/4280763003.py\", line 22, in run_sweep_trial\n",
      "  File \"/tmp/ipykernel_25416/2469710868.py\", line 52, in train\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1056, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 151, in run\n",
      "    self.on_advance_end(data_fetcher)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 370, in on_advance_end\n",
      "    self.val_loop.run()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 451, in _evaluation_step\n",
      "    call._call_callback_hooks(trainer, hook_name, output, *hook_kwargs.values())\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py\", line 227, in _call_callback_hooks\n",
      "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/tqdm_progress.py\", line 326, in on_validation_batch_end\n",
      "    _update_n(self.val_progress_bar, n)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/pytorch_lightning/callbacks/progress/tqdm_progress.py\", line 459, in _update_n\n",
      "    bar.refresh()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/tqdm/std.py\", line 1347, in refresh\n",
      "    self.display()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/tqdm/notebook.py\", line 157, in display\n",
      "    pbar.value = self.n\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/traitlets/traitlets.py\", line 716, in __set__\n",
      "    self.set(obj, value)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/traitlets/traitlets.py\", line 706, in set\n",
      "    obj._notify_trait(self.name, old_value, new_value)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/traitlets/traitlets.py\", line 1513, in _notify_trait\n",
      "    self.notify_change(\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipywidgets/widgets/widget.py\", line 700, in notify_change\n",
      "    self.send_state(key=name)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipywidgets/widgets/widget.py\", line 586, in send_state\n",
      "    self._send(msg, buffers=buffers)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipywidgets/widgets/widget.py\", line 825, in _send\n",
      "    self.comm.send(data=msg, buffers=buffers)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/comm/base_comm.py\", line 147, in send\n",
      "    self.publish_msg(\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/comm/comm.py\", line 37, in publish_msg\n",
      "    self.kernel.session.send(\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/jupyter_client/session.py\", line 863, in send\n",
      "    stream.send_multipart(to_send, copy=copy)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 346, in send_multipart\n",
      "    return self.io_thread.send_multipart(*args, **kwargs)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 276, in send_multipart\n",
      "    self.schedule(lambda: self._really_send(*args, **kwargs))\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_25416/4280763003.py\", line 36, in run_sweep_trial\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 3613, in __exit__\n",
      "    traceback.print_exception(exc_type, exc_val, exc_tb)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/traceback.py\", line 105, in print_exception\n",
      "    print(line, file=file, end=\"\")\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/lib/console_capture.py\", line 147, in write_with_callbacks\n",
      "    n = orig_write(s)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 694, in write\n",
      "    self._schedule_flush()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 590, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 2352, in _atexit_cleanup\n",
      "    self._on_finish()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 2604, in _on_finish\n",
      "    with progress.progress_printer(\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/contextlib.py\", line 119, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/lib/progress.py\", line 92, in progress_printer\n",
      "    with printer.dynamic_text() as text_area:\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/contextlib.py\", line 119, in __enter__\n",
      "    return next(self.gen)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/lib/printer.py\", line 436, in dynamic_text\n",
      "    handle = self._ipython_display.display(\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/IPython/core/display_functions.py\", line 305, in display\n",
      "    publish_display_data(data=format_dict, metadata=md_dict, **kwargs)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/IPython/core/display_functions.py\", line 93, in publish_display_data\n",
      "    display_pub.publish(\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 103, in publish\n",
      "    self._flush_streams()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 66, in _flush_streams\n",
      "    sys.stdout.flush()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 604, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/threading.py\", line 917, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/agents/pyagent.py\", line 311, in _run_job\n",
      "    wandb.finish(exit_code=1)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 4130, in finish\n",
      "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 391, in wrapper\n",
      "    return func(self, *args, **kwargs)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 2106, in finish\n",
      "    return self._finish(exit_code)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 2127, in _finish\n",
      "    self._atexit_cleanup(exit_code=exit_code)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 2362, in _atexit_cleanup\n",
      "    wandb.termerror(\"Problem finishing run\")\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/errors/term.py\", line 213, in termerror\n",
      "    _log(\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/wandb/errors/term.py\", line 358, in _log\n",
      "    click.echo(string, file=sys.stderr, nl=newline)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/click/utils.py\", line 319, in echo\n",
      "    file.flush()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 604, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Exception in threading.excepthook:\n",
      "Exception ignored in thread started by: <bound method Thread._bootstrap of <Thread(Thread-35, stopped 139919875851840)>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/threading.py\", line 937, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/threading.py\", line 982, in _bootstrap_inner\n",
      "    self._invoke_excepthook(self)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/threading.py\", line 1264, in invoke_excepthook\n",
      "    local_print(\"Exception in threading.excepthook:\",\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 604, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 604, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/ipykernel/iostream.py\", line 267, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/home/david/miniconda3/envs/nlp-real/lib/python3.9/site-packages/zmq/sugar/socket.py\", line 701, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"_zmq.py\", line 1092, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1134, in zmq.backend.cython._zmq.Socket.send\n",
      "  File \"_zmq.py\", line 1209, in zmq.backend.cython._zmq._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by -summary_metrics.val_acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "Learning Rate: 1e-05\n",
      "Weight Decay: 0.001\n",
      "Dropout Probability: 0.2\n",
      "Use Layer Norm: True\n",
      "Hidden Size Multiplier: 1\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters from the sweep\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(f\"dhodel-hslu-nlp/hslu-fs25-nlp-qa-transformers/{sweep_id}\")\n",
    "best_run = sweep.best_run()\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_config = best_run.config\n",
    "print(\"Best hyperparameters:\")\n",
    "print(f\"Learning Rate: {best_config['learning_rate']}\")\n",
    "print(f\"Weight Decay: {best_config['weight_decay']}\")\n",
    "print(f\"Dropout Probability: {best_config['dropout_prob']}\")\n",
    "print(f\"Use Layer Norm: {best_config['use_layer_norm']}\")\n",
    "print(f\"Hidden Size Multiplier: {best_config['hidden_size_multiplier']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef440143",
   "metadata": {},
   "source": [
    "### 1) Pre-trained Transformer with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "252c4a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 768\n"
     ]
    }
   ],
   "source": [
    "# Initialize models with the best hyperparameters\n",
    "pretrained_roberta_model = RobertaMultipleChoiceModel(\n",
    "    roberta_model=pretrained_distilroberta, \n",
    "    dropout_prob=best_config['dropout_prob'], \n",
    "    learning_rate=best_config['learning_rate'], \n",
    "    weight_decay=best_config['weight_decay'], \n",
    "    use_layer_norm=best_config['use_layer_norm'], \n",
    "    hidden_size_multiplier=best_config['hidden_size_multiplier'],\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cfe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_checkpoints_path = \"./checkpoints/pretrained\"\n",
    "os.makedirs(pretrained_checkpoints_path, exist_ok=True)\n",
    "\n",
    "pretrained_roberta_trainer, pretrained_roberta_best_checkpoint = train(\n",
    "  model=pretrained_roberta_model,\n",
    "  data_module=data_module,\n",
    "  max_epochs=50,\n",
    "  checkpoints_path=pretrained_checkpoints_path,\n",
    "  early_stopping_patience=5,\n",
    "  wandb_run_prefix=\"pretrained-roberta-best-params\",\n",
    "  debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13610be",
   "metadata": {},
   "source": [
    "### 2) Randomly Initialized Transformer with Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the randomly initialized model with the same best hyperparameters\n",
    "random_initialized_roberta_model = RobertaMultipleChoiceModel(\n",
    "    roberta_model=random_initialized_roberta, \n",
    "    dropout_prob=best_config['dropout_prob'], \n",
    "    learning_rate=best_config['learning_rate'], \n",
    "    weight_decay=best_config['weight_decay'], \n",
    "    use_layer_norm=best_config['use_layer_norm'], \n",
    "    hidden_size_multiplier=best_config['hidden_size_multiplier'],\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076bce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_initialized_checkpoints_path = \"./checkpoints/random-initialized\"\n",
    "os.makedirs(random_initialized_checkpoints_path, exist_ok=True)\n",
    "\n",
    "randomly_initialized_roberta_trainer, randomly_initialized_roberta_best_checkpoint = train(\n",
    "  model=random_initialized_roberta_model,\n",
    "  data_module=data_module,\n",
    "  max_epochs=50,\n",
    "  checkpoints_path=random_initialized_checkpoints_path,\n",
    "  early_stopping_patience=5,\n",
    "  wandb_run_prefix=\"random-initialized-roberta-best-params\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3796cceb",
   "metadata": {},
   "source": [
    "### 3) Prompt Engineering with Phi LLM\n",
    "\n",
    "For the Phi model, we don't need training since we're using prompt engineering.\n",
    "\n",
    "For trying out different prompt, I run the mode against a subset of the validation set as the trend of the performance emerges quickly.\n",
    "\n",
    "What I've tried:\n",
    "- Give between 3 and 6 example questions and answers\n",
    "- Include easier and harder examples\n",
    "- Add clarity about tie-breaking: `If multiple options seem correct, choose the MOST appropriate or complete answer`\n",
    "- Chain-of-thought prompting\n",
    "- Encourage double-checking: `Before submitting your answer, verify it makes logical sense and is the most appropriate choice for the question.`\n",
    "\n",
    "In conclusion, the simple prompt with just 2 examples to tune it to the task seems to work best.\n",
    "All othger additions made only a small difference, mostly negative.\n",
    "The chain-of-thought prompting made the evaluating much slower (factor ~50) due to the increased number of tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc8dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_prompt_engineering = PhiPromptEngineering(\n",
    "    model=phi_model,\n",
    "    tokenizer=phi_tokenizer,\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ee292",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    entity=\"dhodel-hslu-nlp\",\n",
    "    project=\"hslu-fs25-nlp-qa-transformers\",\n",
    "    name=f\"phi-prompt-engineering-{datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\",\n",
    "    reinit=True,\n",
    "    config={\n",
    "        \"model_name\": phi_model_name,\n",
    "        \"prompt_template\": phi_prompt_valid.prompt_template,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcce2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_accuracy, y, y_pred = phi_prompt_engineering.evaluate(phi_prompt_valid, log_wandb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb55dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff193822",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f338b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_roberta_trainer.logger = False\n",
    "randomly_initialized_roberta_trainer.logger = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d53eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_roberta_model.reset_test_arrays()\n",
    "pretrained_test_results = pretrained_roberta_trainer.test(pretrained_roberta_model, datamodule=data_module, ckpt_path=pretrained_roberta_best_checkpoint)\n",
    "\n",
    "random_initialized_roberta_model.reset_test_arrays()\n",
    "random_test_results = randomly_initialized_roberta_trainer.test(random_initialized_roberta_model, datamodule=data_module, ckpt_path=randomly_initialized_roberta_best_checkpoint)\n",
    "\n",
    "pretrained_test_labels = pretrained_roberta_model.test_y\n",
    "pretrained_test_preds = pretrained_roberta_model.test_y_pred\n",
    "\n",
    "random_test_labels = random_initialized_roberta_model.test_y\n",
    "random_test_preds = random_initialized_roberta_model.test_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_accuracy, phi_test_labels, phi_test_preds = phi_prompt_engineering.evaluate(phi_prompt_test, log_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {i: chr(65 + i) for i in range(5)}  # 0->A, 1->B, etc.\n",
    "label_names = list(label_mapping.values())\n",
    "\n",
    "pretrained_cm = confusion_matrix(pretrained_test_labels, pretrained_test_preds)\n",
    "random_cm = confusion_matrix(random_test_labels, random_test_preds)\n",
    "phi_cm = confusion_matrix(phi_test_labels, phi_test_preds)\n",
    "\n",
    "# Determine the global min and max values for consistent scaling\n",
    "global_vmin = min(pretrained_cm.min(), random_cm.min(), phi_cm.min())\n",
    "global_vmax = max(pretrained_cm.max(), random_cm.max(), phi_cm.max())\n",
    "\n",
    "# Create a figure with three subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "# Plot confusion matrices\n",
    "sns.heatmap(pretrained_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=label_names, yticklabels=label_names, \n",
    "            ax=ax1, vmin=global_vmin, vmax=global_vmax)\n",
    "ax1.set_title(\"Pretrained RoBERTa Model Confusion Matrix\", fontsize=14)\n",
    "ax1.set_xlabel(\"Predicted Choice\", fontsize=12)\n",
    "ax1.set_ylabel(\"True Choice\", fontsize=12)\n",
    "\n",
    "sns.heatmap(random_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=label_names, yticklabels=label_names, \n",
    "            ax=ax2, vmin=global_vmin, vmax=global_vmax)\n",
    "ax2.set_title(\"Random Initialized RoBERTa Model Confusion Matrix\", fontsize=14)\n",
    "ax2.set_xlabel(\"Predicted Choice\", fontsize=12)\n",
    "ax2.set_ylabel(\"True Choice\", fontsize=12)\n",
    "\n",
    "sns.heatmap(phi_cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "            xticklabels=label_names, yticklabels=label_names, \n",
    "            ax=ax3, vmin=global_vmin, vmax=global_vmax)\n",
    "ax3.set_title(\"Phi-4-mini Confusion Matrix\", fontsize=14)\n",
    "ax3.set_xlabel(\"Predicted Choice\", fontsize=12)\n",
    "ax3.set_ylabel(\"True Choice\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphics/confusion_matrices_all_models.svg\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Create a bar chart to compare model performance\n",
    "model_names = ['Pretrained RoBERTa', 'Random Initialized RoBERTa', 'Phi-4-mini']\n",
    "accuracies = [\n",
    "    pretrained_test_results[0]['test_acc'],\n",
    "    random_test_results[0]['test_acc'],\n",
    "    phi_accuracy\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Model Performance Comparison', fontsize=14)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "\n",
    "# Add the accuracy values on top of the bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., \n",
    "             bar.get_height() + 0.01, \n",
    "             f'{acc:.4f}', \n",
    "             ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('graphics/model_comparison.svg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57b123",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8915d4",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "\n",
    "The comparison between our three models - pre-trained RoBERTa, randomly initialized RoBERTa, and Phi-4-mini LLM (with prompt engineering) - reveals several interesting patterns in their performance on the CommonsenseQA task.\n",
    "\n",
    "1. **Pre-trained RoBERTa**: The model leverages transfer learning from its pre-training phase, giving it a strong foundation for understanding language patterns and semantics before fine-tuning on our specific task.\n",
    "\n",
    "2. **Randomly Initialized RoBERTa**: Starting from scratch, this model had to learn language patterns solely from our training data, which is much more challenging given the limited size of the dataset compared to typical pre-training datasets.\n",
    "\n",
    "3. **Phi-4-mini LLM (Prompt Engineering)**: This approach uses a much larger model (7B parameters) without any task-specific fine-tuning, relying instead on prompt engineering to elicit the desired behavior.\n",
    "\n",
    "The confusion matrices reveal each model's specific strengths and weaknesses in predicting different answer choices. The bar chart provides a clear comparison of overall accuracy between the three approaches.\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- **Effect of Pre-training**: The significant performance gap between the pre-trained and randomly initialized models demonstrates the value of transfer learning, especially for tasks with limited labeled data.\n",
    "\n",
    "- **LLM with Prompt Engineering**: Phi's performance shows how effectively large language models can be adapted to specific tasks without fine-tuning, using only careful prompt design.\n",
    "\n",
    "- **Error Patterns**: The confusion matrices show different patterns of errors across models, suggesting they may be making different types of mistakes despite being evaluated on the same task.\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "This comparison illustrates the trade-offs between different approaches to transformer-based question answering:\n",
    "\n",
    "- Pre-trained + fine-tuned models offer strong performance with reasonable computational requirements\n",
    "- Randomly initialized models struggle without transfer learning benefits\n",
    "- Large LLMs with prompt engineering can achieve competitive results without task-specific training, but at higher computational cost\n",
    "\n",
    "The results highlight how different transformer-based approaches can be selected based on available resources, performance requirements, and deployment constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c979c0",
   "metadata": {},
   "source": [
    "## Tools Used\n",
    "\n",
    "- Visual Studio Code as IDE\n",
    "- Jupyter Notebook for interactive development\n",
    "- Python 3.9.21\n",
    "- GitHub for version control\n",
    "- Weights & Biases for experiment tracking and hyperparameter optimization\n",
    "- Claude 3.7 Sonnet for troubleshooting, finding bugs and discussing ideas\n",
    "- Github Copilot Chat for troubleshooting and finding bugs"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "nlp-real",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
