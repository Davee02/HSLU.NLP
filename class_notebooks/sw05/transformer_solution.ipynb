{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2fab24",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "We will implement the Transformer architecture presented in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f86ec2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b41c4",
   "metadata": {},
   "source": [
    "We start with the attention. Define a class `TransformerAttention` that will contain all the functions related to the Transformer's attention that we need. Add an `__init__` method that takes `hidden_dim` and `num_heads` as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acea5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07441c1e",
   "metadata": {},
   "source": [
    "Now we're adding its functions one after the other. We start with the best part: the attention function. Implement scaled-dot product attention when given `query`, `key`, and `value` tensors as inputs. The dimensions of these tensors are: `[batch_size, sequence_length, head_dim]`. Scaled dot-product attention is defined as:\n",
    "$$\\text{DPA}(Q, K, V) = \\text{softmax}(\\frac{Q K^\\top}{\\sqrt{d}}) V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d266fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dot_product_attention(self, query, key, value):\n",
    "    head_dim = query.size(-1)\n",
    "    attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "    attention_scores = attention_scores / math.sqrt(head_dim)\n",
    "    attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "    return torch.matmul(attention_probs, value)\n",
    "\n",
    "TransformerAttention.dot_product_attention = dot_product_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0397dd",
   "metadata": {},
   "source": [
    "Implement a function `split_to_heads` that takes a tensor of dimensions `[?, ?, hidden_dim]` and splits it into `num_heads` tensors of size `[?, ?, head_dim]`, where $\\text{head\\_dim} = \\frac{\\text{hidden\\_dim}}{\\text{num\\_heads}}$. The `?` dimensions are the same as before, but your implementation should be independent of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acfa006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_heads(self, tensor):\n",
    "    assert self.hidden_dim % self.num_heads == 0, \"Hidden dim needs to be divisible by num_heads\"\n",
    "    head_dim = self.hidden_dim // self.num_heads\n",
    "    # Alternative:\n",
    "    # return tensor.view(tensor.size(0), tensor.size(1), self.num_heads, head_dim).unbind(2)\n",
    "    return tensor.split(head_dim, dim=-1)\n",
    "\n",
    "TransformerAttention.split_to_heads = split_to_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ba143",
   "metadata": {},
   "source": [
    "Now implement the `forward` method of `TransformerAttention` (and extend the `__init__` method if necessary). It should:\n",
    "1. project its inputs into `query`, `key` and `value` tensors with 3 separate linear layers\n",
    "2. split the tensors into chunks for each head to process\n",
    "3. perform attention for each head separately\n",
    "4. concatenate the results\n",
    "5. run the output through another linear layer\n",
    "\n",
    "Step 1 and 2 look reversed from the diagram we saw in class, but this is more intuitive and also how Hugging Face implements these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5af616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, hidden_dim, num_heads):\n",
    "    super(TransformerAttention, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.query_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.key_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.value_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.output_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "def forward(self, x):\n",
    "    query = self.query_projection(x)\n",
    "    key = self.key_projection(x)\n",
    "    value = self.value_projection(x)\n",
    "\n",
    "    # Note: this operation can be vectorized for efficiency:\n",
    "    # Instead of creating `num_heads` tensors, keep 1 tensor and add a dimension of size `num_heads`\n",
    "    head_queries = self.split_to_heads(query)\n",
    "    head_keys = self.split_to_heads(key)\n",
    "    head_values = self.split_to_heads(value)\n",
    "    \n",
    "    attention_outputs = []\n",
    "    for head_query, head_key, head_value in zip(head_queries, head_keys, head_values):\n",
    "        attention_outputs.append(self.dot_product_attention(head_query, head_key, head_value))\n",
    "\n",
    "    attention_output_tensor = torch.cat(attention_outputs, dim=-1)\n",
    "\n",
    "    return self.output_projection(attention_output_tensor)\n",
    "\n",
    "TransformerAttention.__init__ = __init__\n",
    "TransformerAttention.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca8c8c",
   "metadata": {},
   "source": [
    "Create a class `TransformerAttentionBlock` that runs Transformer attention, then adds the input as a residual to the output and performs layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b3949dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = TransformerAttention(hidden_dim, num_heads)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.attention(x)\n",
    "        return self.layer_norm(x + output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a1712a",
   "metadata": {},
   "source": [
    "Create a class `FeedForwardNetwork` that consists of two linear layers with a ReLU in between. Also add a residual connection from the input to the output and apply layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5130273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, inner_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, inner_dim)\n",
    "        self.linear2 = nn.Linear(inner_dim, hidden_dim)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.linear1(x)\n",
    "        output = F.relu(output)\n",
    "        output = self.linear2(output)\n",
    "        return self.layer_norm(x + output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051c66b",
   "metadata": {},
   "source": [
    "Now we can combine the `TransformerAttentionBlock` and the `FeedForwardNetwork` into a `TransformerLayer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f85aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ffn_inner_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_attention = TransformerAttentionBlock(hidden_dim, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(hidden_dim, ffn_inner_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.self_attention(x)\n",
    "        out = self.ffn(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678fa37",
   "metadata": {},
   "source": [
    "We are ready to compose our `TransformerEncoder` of a given number of `TransformerLayer`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2d99df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ffn_inner_dim, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerLayer(hidden_dim, ffn_inner_dim, num_heads) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8038dc",
   "metadata": {},
   "source": [
    "Let's test our implementation with the hyperparameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6eec7019",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 20\n",
    "embedding_dim = hidden_dim\n",
    "ffn_dim = 100\n",
    "num_heads = 4\n",
    "num_encoder_layers = 6\n",
    "batch_size = 2\n",
    "x_len = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dfc807",
   "metadata": {},
   "source": [
    "... and check if it produces the correct output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f5476d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, x_len, embedding_dim)\n",
    "encoder = TransformerEncoder(hidden_dim, ffn_dim, num_encoder_layers, num_heads)\n",
    "output = encoder(x)\n",
    "assert list(output.shape) == [batch_size, x_len, hidden_dim], \"Wrong output shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f341c2",
   "metadata": {},
   "source": [
    "## Transformer Decoder\n",
    "For the Transformer decoder, two components are missing.\n",
    "1. A causal mask in the `TransformerAttention`.\n",
    "2. A cross-attention module in the `TransformerLayer`.\n",
    "\n",
    "We start by generalizing the `TransformerAttention` class to use a causal mask in `dot_product_attention` if it is used for decoder self-attention. We check this by accessing an `is_decoder_self_attention` attribute of `self`, which we have to add as an argument to `TransformerAttention`'s `__init__` method first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "662da648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an `is_decoder_self_attention` attribute to TransformerAttention.__init__\n",
    "def __init__(self, hidden_dim, num_heads, is_decoder_self_attention=False):\n",
    "    super(TransformerAttention, self).__init__()\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.is_decoder_self_attention = is_decoder_self_attention\n",
    "    self.query_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.key_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.value_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "    self.output_projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "TransformerAttention.__init__ = __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed44eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the dot_product attention to use a causal mask in case it is used in the decoder self-attention.\n",
    "def dot_product_attention(self, query, key, value):\n",
    "        head_dim = query.size(-1)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(head_dim)\n",
    "        if self.is_decoder_self_attention:\n",
    "            batch_size, seq_length, _ = query.size()\n",
    "            \n",
    "            # the causal mask is a lower triangular matrix (see lecture slides)\n",
    "            # it is 0 for attentions into the future, 1 otherwise\n",
    "            causal_mask = torch.tril(torch.ones(batch_size, seq_length, seq_length))\n",
    "            \n",
    "            # or: HuggingFace's implementation of a causal mask (same result)\n",
    "            seq_ids = torch.arange(seq_length)\n",
    "            causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
    "            causal_mask = causal_mask.to(torch.float32)  # convert to float for next operations\n",
    "            \n",
    "            # to apply it to the attention scores, which can be in (-inf, inf),\n",
    "            # we subtract a large number from the scores that would see the future\n",
    "            # such that they become very small => probabilities in softmax become 0\n",
    "            scores_to_mask = 1 - causal_mask\n",
    "            attention_scores = attention_scores - scores_to_mask * 1e8\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        return torch.matmul(attention_probs, value)\n",
    "\n",
    "TransformerAttention.dot_product_attention = dot_product_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba48c06",
   "metadata": {},
   "source": [
    "Now we add cross-attention. We do this by updating the `TransformerAttention`'s `forward` method to take `encoder_hidden_states` as an optional input. Check the lecture slides to see which input gets projected into queries, keys and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b5a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, encoder_hidden_states=None):\n",
    "    query = self.query_projection(x)\n",
    "    \n",
    "    # In cross_attention, keys and values come from encoder_hidden_states.\n",
    "    if encoder_hidden_states is not None:\n",
    "        key = self.key_projection(encoder_hidden_states)\n",
    "        value = self.value_projection(encoder_hidden_states)\n",
    "    else:\n",
    "        key = self.key_projection(x)\n",
    "        value = self.value_projection(x)\n",
    "\n",
    "    head_queries = self.split_to_heads(query)\n",
    "    head_keys = self.split_to_heads(key)\n",
    "    head_values = self.split_to_heads(value)\n",
    "    attention_outputs = []\n",
    "    for head_query, head_key, head_value in zip(head_queries, head_keys, head_values):\n",
    "        attention_outputs.append(self.dot_product_attention(head_query, head_key, head_value))\n",
    "    attention_output_tensor = torch.cat(attention_outputs, dim=-1)\n",
    "    return self.output_projection(attention_output_tensor)\n",
    "    \n",
    "TransformerAttention.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33f0aed",
   "metadata": {},
   "source": [
    "We have to extend the `TransformerAttentionBlock` to allow that additional argument in its `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "987baf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x, encoder_hidden_states=None):\n",
    "    output = self.attention(x, encoder_hidden_states)\n",
    "    return self.layer_norm(x + output)\n",
    "\n",
    "TransformerAttentionBlock.forward = forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1c6a5",
   "metadata": {},
   "source": [
    "Now we implement a `TransformerDecoderLayer` that consists of decoder self-attention, cross-attention and a feed-forward network. In the `forward` method, use the encoder hidden states as inputs to the cross-attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c3f2d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ffn_inner_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.self_attention = TransformerAttentionBlock(hidden_dim, num_heads)\n",
    "        self.self_attention.attention.is_decoder_self_attention = True\n",
    "        self.cross_attention = TransformerAttentionBlock(hidden_dim, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(hidden_dim, ffn_inner_dim)\n",
    "    \n",
    "    def forward(self, x, encoder_hidden_states):\n",
    "        out = self.self_attention(x)\n",
    "        out = self.cross_attention(out, encoder_hidden_states)\n",
    "        out = self.ffn(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9133819c",
   "metadata": {},
   "source": [
    "Add a `TransformerDecoder` that holds the decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c00668ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ffn_inner_dim, num_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(hidden_dim, ffn_inner_dim, num_heads) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x, encoder_hidden_states):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_hidden_states)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50442dd4",
   "metadata": {},
   "source": [
    "## Transformer Seq2seq Model\n",
    "We can now put everything together. Create and instantiate a Transformer model that encodes a random input `x`, then generates an output hidden representation for each decoder input `y` that we could then feed into a classifier to predict the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e31aa870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, ffn_dim, num_encoder_layers, num_decoder_layers, num_heads):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(hidden_dim, ffn_dim, num_encoder_layers, num_heads)\n",
    "        self.decoder = TransformerDecoder(hidden_dim, ffn_dim, num_decoder_layers, num_heads)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        encoder_hidden_states = self.encoder(x)\n",
    "        return self.decoder(y, encoder_hidden_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e1ffe",
   "metadata": {},
   "source": [
    "We will use the following hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "503bddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 20\n",
    "embedding_dim = hidden_dim\n",
    "ffn_dim = 100\n",
    "num_heads = 4\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 2\n",
    "batch_size = 2\n",
    "x_len = 10\n",
    "y_len = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d6de3",
   "metadata": {},
   "source": [
    "Now we can run our model and test that the output dimensions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4caa796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, x_len, embedding_dim)\n",
    "y = torch.randn(batch_size, y_len, embedding_dim)\n",
    "model = TransformerModel(hidden_dim, ffn_dim, num_encoder_layers, num_decoder_layers, num_heads)\n",
    "output = model(x, y)\n",
    "assert list(output.shape) == [batch_size, y_len, hidden_dim], \"Wrong output shape\"\n",
    "num_model_params = sum(param.numel() for param in model.parameters())\n",
    "assert num_model_params == 50480, f\"Wrong number of parameters: {num_model_params}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071506d",
   "metadata": {},
   "source": [
    "## What is missing for a real implementation?\n",
    "Look at the [implementation of the Transformer layer for BERT by HuggingFace](https://github.com/huggingface/transformers/blob/v4.46.0/src/transformers/models/bert/modeling_bert.py#L223), from line 223 until 641.\n",
    "\n",
    "**Question:** Name the things you see HuggingFace's implementation do that is still missing in your own implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec33e1b",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "- Batching/masks: when processing a batch in vectorized form, an additional argument `attention_mask` is supplied that masks padded positions\n",
    "- Dropout\n",
    "- Embeddings\n",
    "- Absolute/relative position embeddings\n",
    "- Caching past keys and values for faster generation\n",
    "- Pruning attention heads (advanced technique, for efficiency)\n",
    "- Outputting attention probabilities and hidden states for analysis (e.g. visualization)\n",
    "- Bert model specifics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
