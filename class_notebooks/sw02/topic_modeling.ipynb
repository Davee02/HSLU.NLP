{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d81604-025d-4fe1-a130-6a978f5ba135",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "In this exercise, we will do topic modeling with gensim. Use the [topics and transformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45876ae-0f77-4bf8-8da4-b18618005327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0afb6f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6efd1",
   "metadata": {},
   "source": [
    "For tokenizing words and stopword removal, download the NLTK punkt tokenizer and stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edf524f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/david/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee84f40-20bf-47da-b0b4-a0ff28f9b5cd",
   "metadata": {},
   "source": [
    "First, we load the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) included with gensim that contains 300 news articles of the Australian Broadcasting Corporation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24d72e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "train_file = datapath('lee_background.cor')\n",
    "articles_orig = open(train_file).read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2e56f",
   "metadata": {},
   "source": [
    "Preprocess the text by lowercasing, removing stopwords, stemming, and removing rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88a870af-9f6b-43ea-940f-558e9a21bb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 09:12:38,210 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2025-02-28 09:12:38,233 : INFO : built Dictionary<7349 unique tokens: [\"'ve\", ',', '.', '100', '4:00pm']...> from 300 documents (total 40467 corpus positions)\n",
      "2025-02-28 09:12:38,234 : INFO : Dictionary lifecycle event {'msg': 'built Dictionary<7349 unique tokens: [\"\\'ve\", \\',\\', \\'.\\', \\'100\\', \\'4:00pm\\']...> from 300 documents (total 40467 corpus positions)', 'datetime': '2025-02-28T09:12:38.234297', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:24:11) \\n[GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'created'}\n",
      "2025-02-28 09:12:38,238 : INFO : discarding 3829 tokens: [(',', 294), ('.', 300), ('associated', 1), ('burn', 1), ('claire', 1), ('cranebrook', 1), ('deterioration', 1), ('directions', 1), ('falls', 1), ('finger', 1)]...\n",
      "2025-02-28 09:12:38,239 : INFO : keeping 3520 tokens which were in no less than 2 and no more than 150 (=50.0%) documents\n",
      "2025-02-28 09:12:38,242 : INFO : resulting dictionary: Dictionary<3520 unique tokens: [\"'ve\", '100', '4:00pm', '500', '60']...>\n"
     ]
    }
   ],
   "source": [
    "# define stopword list\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = stopwords | {'\\\"', '\\'', '\\'\\'', '`', '``', '\\'s'}\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(article, stem=False):\n",
    "    # tokenize\n",
    "    article = nltk.word_tokenize(article)\n",
    "\n",
    "    # lowercase all words\n",
    "    article = [word.lower() for word in article]\n",
    "\n",
    "    # remove stopwords\n",
    "    article = [word for word in article if word not in stopwords]\n",
    "\n",
    "    # optional: stem\n",
    "    if stem:\n",
    "        article = [stemmer.stem(word) for word in article]\n",
    "    return article\n",
    "\n",
    "articles = [preprocess(article) for article in articles_orig]\n",
    "\n",
    "# create the dictionary and corpus objects that gensim uses for topic modeling\n",
    "dictionary = gensim.corpora.Dictionary(articles)\n",
    "\n",
    "# remove words that occur in less than 2 documents, or more than 50% of documents\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "temp = dictionary[0]  # load the dictionary by calling it once\n",
    "corpus_bow = [dictionary.doc2bow(article) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ae61a",
   "metadata": {},
   "source": [
    "\n",
    "Now we create a TF-IDF model and transform the corpus into TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab13db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 09:12:38,264 : INFO : collecting document frequencies\n",
      "2025-02-28 09:12:38,265 : INFO : PROGRESS: processing document #0\n",
      "2025-02-28 09:12:38,274 : INFO : TfidfModel lifecycle event {'msg': 'calculated IDF weights for 300 documents and 3520 features (22934 matrix non-zeros)', 'datetime': '2025-02-28T09:12:38.274473', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:24:11) \\n[GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'initialize'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 7), (42, 1), (43, 1), (44, 1), (45, 3), (46, 1), (47, 1), (48, 2), (49, 2), (50, 3), (51, 3), (52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 8), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 3), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 5), (90, 1), (91, 2), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 3), (99, 1), (100, 1), (101, 3), (102, 1), (103, 1), (104, 1), (105, 4), (106, 2), (107, 1), (108, 1), (109, 1), (110, 1)]\n",
      "[(0, 0.045163832296308125), (1, 0.049004990699027966), (2, 0.09398031720792203), (3, 0.06797874731615453), (4, 0.08637534553463991), (5, 0.10158528888120417), (6, 0.058872481173046734), (7, 0.045871696227162966), (8, 0.04660732651093343), (9, 0.03476708703034139), (10, 0.09174339245432593), (11, 0.06379342938648586), (12, 0.08097953226203827), (13, 0.08637534553463991), (14, 0.06576958891547403), (15, 0.05748249959948285), (16, 0.07679421433236962), (17, 0.09398031720792203), (18, 0.04197717742438698), (19, 0.06379342938648586), (20, 0.09398031720792203), (21, 0.07679421433236962), (22, 0.08097953226203827), (23, 0.058872481173046734), (24, 0.05497796237027076), (25, 0.05497796237027076), (26, 0.07337456058875615), (27, 0.05497796237027076), (28, 0.08637534553463991), (29, 0.058872481173046734), (30, 0.06200577564491172), (31, 0.08637534553463991), (32, 0.09398031720792203), (33, 0.047372990696988616), (34, 0.07048328454536662), (35, 0.09398031720792203), (36, 0.09398031720792203), (37, 0.07679421433236962), (38, 0.06379342938648586), (39, 0.09398031720792203), (40, 0.05276880396959025), (41, 0.3161468260741569), (42, 0.06576958891547403), (43, 0.06576958891547403), (44, 0.04197717742438698), (45, 0.18601732693473516), (46, 0.08637534553463991), (47, 0.09398031720792203), (48, 0.17275069106927982), (49, 0.15358842866473923), (50, 0.1973087667464221), (51, 0.19138028815945754), (52, 0.06379342938648586), (53, 0.18796063441584407), (54, 0.07679421433236962), (55, 0.053840876780419114), (56, 0.07679421433236962), (57, 0.07679421433236962), (58, 0.08637534553463991), (59, 0.043187672767319954), (60, 0.13595749463230905), (61, 0.07048328454536662), (62, 0.06797874731615453), (63, 0.043187672767319954), (64, 0.08637534553463991), (65, 0.04448171465359908), (66, 0.049877527926200725), (67, 0.07337456058875615), (68, 0.05175471008582299), (69, 0.029876861457627475), (70, 0.043823535964961836), (71, 0.07337456058875615), (72, 0.1663540992526395), (73, 0.048171245973727274), (74, 0.09398031720792203), (75, 0.06200577564491172), (76, 0.04274284161044218), (77, 0.07337456058875615), (78, 0.06037377564287238), (79, 0.18796063441584407), (80, 0.09398031720792203), (81, 0.06379342938648586), (82, 0.23038264299710884), (83, 0.05618845771320373), (84, 0.08097953226203827), (85, 0.06379342938648586), (86, 0.07048328454536662), (87, 0.053840876780419114), (88, 0.06797874731615453), (89, 0.14342796675805272), (90, 0.07679421433236962), (91, 0.10995592474054151), (92, 0.06379342938648586), (93, 0.03976801902370649), (94, 0.0360042057531442), (95, 0.06797874731615453), (96, 0.07679421433236962), (97, 0.058872481173046734), (98, 0.11930405707111948), (99, 0.07679421433236962), (100, 0.03050212495565461), (101, 0.18601732693473516), (102, 0.05618845771320373), (103, 0.058872481173046734), (104, 0.08097953226203827), (105, 0.17529414385984735), (106, 0.11237691542640746), (107, 0.045871696227162966), (108, 0.08097953226203827), (109, 0.06037377564287238), (110, 0.03398546693692743)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "\n",
    "tfidf = models.TfidfModel(corpus_bow)\n",
    "corpus_tfidf = tfidf[corpus_bow]\n",
    "\n",
    "print(corpus_bow[0])\n",
    "print(corpus_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24df8cb",
   "metadata": {},
   "source": [
    "Now we train an [LDA model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html) with 10 topics on the TF-IDF corpus. Save it to a variable `model_lda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ded6b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 09:12:38,283 : INFO : using symmetric alpha at 0.02\n",
      "2025-02-28 09:12:38,284 : INFO : using symmetric eta at 0.02\n",
      "2025-02-28 09:12:38,285 : INFO : using serial LDA version on this node\n",
      "2025-02-28 09:12:38,298 : INFO : running online (single-pass) LDA training, 50 topics, 1 passes over the supplied corpus of 300 documents, updating model once every 300 documents, evaluating perplexity every 300 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2025-02-28 09:12:38,299 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2025-02-28 09:12:38,516 : INFO : -203.990 per-word bound, 25528097335895521396288743951455578835329263826604725240856576.0 perplexity estimate based on a held-out corpus of 300 documents with 2210 words\n",
      "2025-02-28 09:12:38,518 : INFO : PROGRESS: pass 0, at document #300/300\n",
      "2025-02-28 09:12:38,776 : INFO : topic #7 (0.020): 0.005*\"markets\" + 0.004*\"consumers\" + 0.004*\"man\" + 0.004*\"powell\" + 0.004*\"companies\" + 0.004*\"food\" + 0.004*\"williams\" + 0.004*\"pilot\" + 0.003*\"1998\" + 0.003*\"genetically\"\n",
      "2025-02-28 09:12:38,777 : INFO : topic #16 (0.020): 0.005*\"oil\" + 0.004*\"qantas\" + 0.004*\"pacific\" + 0.003*\"dr\" + 0.003*\"workers\" + 0.003*\"explosives\" + 0.003*\"refugees\" + 0.003*\"airline\" + 0.003*\"island\" + 0.003*\"weapons\"\n",
      "2025-02-28 09:12:38,779 : INFO : topic #4 (0.020): 0.005*\"timor\" + 0.004*\"guilty\" + 0.004*\"gas\" + 0.003*\"project\" + 0.003*\"phillips\" + 0.003*\"east\" + 0.003*\"lead\" + 0.003*\"tyco\" + 0.003*\"nautical\" + 0.003*\"arafat\"\n",
      "2025-02-28 09:12:38,779 : INFO : topic #14 (0.020): 0.004*\"zimbabwe\" + 0.004*\"commonwealth\" + 0.004*\"macfarlane\" + 0.004*\"israel\" + 0.003*\"building\" + 0.003*\"hamas\" + 0.003*\"commission\" + 0.003*\"focus\" + 0.002*\"industry\" + 0.002*\"ministers\"\n",
      "2025-02-28 09:12:38,780 : INFO : topic #39 (0.020): 0.005*\"scale\" + 0.004*\"cut\" + 0.004*\"hollingworth\" + 0.004*\"governor-general\" + 0.004*\"economy\" + 0.004*\"karzai\" + 0.003*\"dr\" + 0.003*\"bank\" + 0.003*\"reserve\" + 0.003*\"woke\"\n",
      "2025-02-28 09:12:38,781 : INFO : topic diff=41.755970, rho=1.000000\n",
      "2025-02-28 09:12:38,783 : INFO : LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=3520, num_topics=50, decay=0.5, chunksize=2000> in 0.48s', 'datetime': '2025-02-28T09:12:38.783017', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:24:11) \\n[GCC 11.2.0]', 'platform': 'Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 50\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model_lda = LdaModel(\n",
    "    corpus=corpus_tfidf,\n",
    "    id2word=id2word,\n",
    "    num_topics=num_topics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91845654",
   "metadata": {},
   "source": [
    "Let's inspect the first 5 topics of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca3a357e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 09:12:38,800 : INFO : topic #45 (0.020): 0.005*\"suharto\" + 0.005*\"ses\" + 0.004*\"wind\" + 0.004*\"doctors\" + 0.003*\"blue\" + 0.003*\"condition\" + 0.003*\"hour\" + 0.003*\"received\" + 0.003*\"warning\" + 0.003*\"mountains\"\n",
      "2025-02-28 09:12:38,803 : INFO : topic #31 (0.020): 0.003*\"claim\" + 0.003*\"places\" + 0.003*\"palestinian\" + 0.003*\"hectares\" + 0.003*\"darwin\" + 0.003*\"native\" + 0.002*\"title\" + 0.002*\"weapons\" + 0.002*\"plants\" + 0.002*\"abc\"\n",
      "2025-02-28 09:12:38,811 : INFO : topic #49 (0.020): 0.005*\"state\" + 0.004*\"south\" + 0.003*\"mounting\" + 0.003*\"india\" + 0.003*\"economy\" + 0.003*\"detainees\" + 0.003*\"test\" + 0.003*\"sydney\" + 0.003*\"industrial\" + 0.003*\"phillips\"\n",
      "2025-02-28 09:12:38,816 : INFO : topic #40 (0.020): 0.005*\"space\" + 0.004*\"endeavour\" + 0.004*\"japanese\" + 0.004*\"report\" + 0.003*\"station\" + 0.003*\"boat\" + 0.003*\"crew\" + 0.003*\"shuttle\" + 0.003*\"company\" + 0.003*\"board\"\n",
      "2025-02-28 09:12:38,831 : INFO : topic #7 (0.020): 0.005*\"markets\" + 0.004*\"consumers\" + 0.004*\"man\" + 0.004*\"powell\" + 0.004*\"companies\" + 0.004*\"food\" + 0.004*\"williams\" + 0.004*\"pilot\" + 0.003*\"1998\" + 0.003*\"genetically\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(45,\n",
       "  '0.005*\"suharto\" + 0.005*\"ses\" + 0.004*\"wind\" + 0.004*\"doctors\" + 0.003*\"blue\" + 0.003*\"condition\" + 0.003*\"hour\" + 0.003*\"received\" + 0.003*\"warning\" + 0.003*\"mountains\"'),\n",
       " (31,\n",
       "  '0.003*\"claim\" + 0.003*\"places\" + 0.003*\"palestinian\" + 0.003*\"hectares\" + 0.003*\"darwin\" + 0.003*\"native\" + 0.002*\"title\" + 0.002*\"weapons\" + 0.002*\"plants\" + 0.002*\"abc\"'),\n",
       " (49,\n",
       "  '0.005*\"state\" + 0.004*\"south\" + 0.003*\"mounting\" + 0.003*\"india\" + 0.003*\"economy\" + 0.003*\"detainees\" + 0.003*\"test\" + 0.003*\"sydney\" + 0.003*\"industrial\" + 0.003*\"phillips\"'),\n",
       " (40,\n",
       "  '0.005*\"space\" + 0.004*\"endeavour\" + 0.004*\"japanese\" + 0.004*\"report\" + 0.003*\"station\" + 0.003*\"boat\" + 0.003*\"crew\" + 0.003*\"shuttle\" + 0.003*\"company\" + 0.003*\"board\"'),\n",
       " (7,\n",
       "  '0.005*\"markets\" + 0.004*\"consumers\" + 0.004*\"man\" + 0.004*\"powell\" + 0.004*\"companies\" + 0.004*\"food\" + 0.004*\"williams\" + 0.004*\"pilot\" + 0.003*\"1998\" + 0.003*\"genetically\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ce453",
   "metadata": {},
   "source": [
    "We see the 5 topics with the highest importance. For each topic, the 10 most important words are shown, together with their coefficient of \"alignment\" to the topic.\n",
    "\n",
    "## Document Similarity\n",
    "We now use our LDA model to compare the similarity of new documents (*queries*) to documents in our collection.\n",
    "\n",
    "First, create an index of the news articles in our corpus. Use the `MatrixSimilarity` transformation as described in gensim's [similarity queries tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eb44cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 09:12:38,843 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2025-02-28 09:12:39,129 : INFO : creating matrix with 300 documents and 50 features\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(model_lda[corpus_tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b2c1f",
   "metadata": {},
   "source": [
    "Now, write a function that takes a query string as input and returns the LDA representation for it. Make sure to apply the same preprocessing as we did to the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dabf9dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(21, 0.3230583), (35, 0.15599045), (44, 0.17489156)]\n"
     ]
    }
   ],
   "source": [
    "def get_lda_representation(query_preprocessed):\n",
    "    query_preprocessed = preprocess(query_preprocessed)\n",
    "    query_bow = dictionary.doc2bow(query_preprocessed)\n",
    "    query_tfidf = tfidf[query_bow]\n",
    "    query_lda = model_lda[query_tfidf]\n",
    "    return query_lda\n",
    "\n",
    "query = \"A new bill sparked massive protests in Israel, as it would massively limit the powers of the judiciary.\"\n",
    "print(get_lda_representation(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77753be",
   "metadata": {},
   "source": [
    "Print the top 5 most similar documents, together with their similarities, using your index created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7696f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.8096423745155334, Article: Russian authorities have sentenced Chechen warlord Salman Raduyev to life in prison for a 1996 hostage siege in which more than 200 people died. Salman Raduyev is probably the most important Chechen fighter Russian authorities have ever caught. A relative of the first Chechen president, he was at the forefront of the insurgency leading raids against federal troops. He was jealous of the achievements of his fellow commanders. He resolved to outperform his rival and in January 1996, masterminded a hostage taking in the neighbouring republic of Dagestan. Apparently, the aim was to destabilise Dagestan and spread the war to the rest of the Caucuses. He ran out of luck as Russian solders were not prepared to negotiate and cornered Raduyev on the Chechen border. \n",
      "Similarity: 0.8096423745155334, Article: Kashmiri militant groups denied involvement in Thursday's attack on the Indian Parliament, accusing Indian intelligence instead. \"We want to make it clear that Kashmiris have no connection with this attack,\" said the Muttahida Jihad Council (MJC), an alliance of 18 groups fighting Indian rule in Kashmir. \"We believe it was carried out by Indian intelligence agencies to achieve their motives about the Kashmir issue,\" the groups added in a statement. The attack on the Parliament building in New Delhi left at least 12 dead. The Indian authorities have not said who they believe was behind the killings. But the Kashmiri groups accused the Indian Government of masterminding the attack in a bid to divert attention from what they called increasing international pressure over Kashmir. \n",
      "Similarity: 0.8096423745155334, Article: Drug education campaigns appear to be paying dividends with new figures showing a 10 per cent drop in drug related deaths last year. According to the Australian Bureau of Statistics, 1,570 people died from drug related causes in the year 2000. That figure is a substantial drop from 1999 when 1,740 Australians died of drug related causes. Across the states and territories, New South Wales recorded the biggest decrease. The bureau's David Payne attributes the decline of drug deaths to the heroin drought in some parts of the country, better equipped ambulances and emergency wards and above all, effective federal and state drug education campaigns. \"They have put a lot of money into the program. \"There has been a fall and while you can't discern a trend from that, the figures are going in the right way, right direction,\" Mr Payne said. \n",
      "Similarity: 0.8096423745155334, Article: A 31-year-old Middle Eastern woman is said to be responding well to treatment after being diagnosed with typhoid in a temporary holding centre on remote Christmas Island. It could be 48 hours before tests can confirm whether the disease has spread further. Two of the woman's three children, a boy aged 13 and a 10-year-old girl, have been quarantined with their mother in the Christmas Island hospital. A third child remains at the island's sports hall, where locals say conditions are crowded and hot. All 540 detainees on Christmas island are being monitored by a health team for signs of fever or abdominal pains, the key symptoms of typhoid, which is spread by contact with contaminated food or water. Hygiene measures have also been stepped up. The Western Australian Health Department is briefing medical staff on infection control procedures but locals have expressed concern the disease could spread to the wider community. \n",
      "Similarity: 0.8096423745155334, Article: The Middle East peace process is under new pressure after an ultimatum from the United States special envoy Anthony Zinni. After two weeks of frustration, he has given Israel and the Palestinians a 48 hour deadline to make some progress or he would go back to Washington. His mission has been accompanied by an upsurge in suicide bombing attacks on Israel and a tough Israeli response. Earlier, Israel rejected a temporary ceasefire offer by four militant Palestinian groups to halt their attacks through to the end of Ramadan next week if Israel agreed to stop assassinating their members. A spokesman for the Government says Israel deals only with the Palestinian Authority and not with terrorist organisations. The conditional ceasefire offer was made by Hamas, Islamic Jihad and the military wing of Yasser Arafat's Fatah faction and follows a sharp upsurge in fighting in the past 10 days. \n"
     ]
    }
   ],
   "source": [
    "query_lda = get_lda_representation(query)\n",
    "sims = index[query_lda]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for doc_position, doc_score in sims[:5]:\n",
    "    print(f\"Similarity: {doc_score}, Article: {articles_orig[doc_position]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e05dba",
   "metadata": {},
   "source": [
    "Run your code again, now training an LDA model with 100 topics. Do you see a qualitative difference in the top-5 most similar documents?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
