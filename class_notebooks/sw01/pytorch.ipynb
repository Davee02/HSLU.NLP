{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f47b1b6f",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "In this exercise, we will look at some basic functionality of PyTorch. Your are free to use other DL frameworks for your exercises and your project. However, the master solutions and code examples will be in PyTorch.\n",
    "\n",
    "The [PyTorch documentation](https://pytorch.org/docs/stable/index.html) offers information on its functionality. A lot of the time, your specific question will also have been asked on the [PyTorch Forum](https://discuss.pytorch.org/), often with competent answers by the core developers (Google will find the relevant thread for you).\n",
    "\n",
    "First, we have to install PyTorch. We will install the basic version for this exercise. For your project, if you want to run on a GPU, you'll have to make sure to have a PyTorch version installed that is compatible with the CUDA version of your NVIDIA drivers. PyTorch has an [installation guide](https://pytorch.org/get-started/locally/) that will help you with getting the right version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a1efca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp313-cp313-linux_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: filelock in /home/david/miniconda3/envs/nlp/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/david/miniconda3/envs/nlp/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/david/miniconda3/envs/nlp/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/david/miniconda3/envs/nlp/lib/python3.13/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/david/miniconda3/envs/nlp/lib/python3.13/site-packages (from torch) (2025.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/david/miniconda3/envs/nlp/lib/python3.13/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/david/miniconda3/envs/nlp/lib/python3.13/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/david/miniconda3/envs/nlp/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/david/miniconda3/envs/nlp/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/david/miniconda3/envs/nlp/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp313-cp313-linux_x86_64.whl (848.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.6/848.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
      "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U numpy\n",
    "%pip install -q ipywidgets \n",
    "%pip install torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c05320f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec5791",
   "metadata": {},
   "source": [
    "## Tensor operations\n",
    "Most of PyTorch's operations have the same name as in NumPy. The basic object for storing data is the `torch.tensor`, the equivalent of the `np.array`. With the help of the [Tensor tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html), do the following:\n",
    "\n",
    "- Create a `torch.tensor` with the elements `[[1, 2], [3, 4]]`\n",
    "- Create a tensor of ones/zeros with the same shape and dtype\n",
    "- Create a random tensor of the same shape\n",
    "- Print the tensor's shape, data type and device\n",
    "- Try to move it to the GPU\n",
    "- For Mac users: Try to move it to [MPS](https://pytorch.org/docs/stable/notes/mps.html)\n",
    "- Check out indexing/slicing operations, and how you can assign values to a slice.\n",
    "- Combine tensors with `torch.cat` and `torch.stack`. What are the differences?\n",
    "- Multiply tensors, element-wise and with matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c90229fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([2, 2])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n",
      "tensor([[1., 0.],\n",
      "        [3., 0.]])\n",
      "tensor([[1., 0.],\n",
      "        [5., 5.]])\n",
      "tensor([[1., 0., 1., 0., 1., 0.],\n",
      "        [5., 5., 5., 5., 5., 5.]])\n",
      "tensor([[1., 0.],\n",
      "        [5., 5.],\n",
      "        [1., 0.],\n",
      "        [5., 5.],\n",
      "        [1., 0.],\n",
      "        [5., 5.]])\n",
      "tensor([[[1., 0.],\n",
      "         [1., 0.],\n",
      "         [1., 0.]],\n",
      "\n",
      "        [[5., 5.],\n",
      "         [5., 5.],\n",
      "         [5., 5.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2], [3, 4]])\n",
    "\n",
    "x_ones = torch.ones_like(x, dtype=x.dtype)\n",
    "x_zeroes = torch.zeros_like(x, dtype=x.dtype)\n",
    "x_random = torch.rand_like(x, dtype=x.dtype)\n",
    "\n",
    "print(f\"Shape of tensor: {x.shape}\")\n",
    "print(f\"Datatype of tensor: {x.dtype}\")\n",
    "print(f\"Device tensor is stored on: {x.device}\")\n",
    "\n",
    "x[:,1] = 0\n",
    "print(x)\n",
    "\n",
    "x[1:] = 5\n",
    "print(x)\n",
    "\n",
    "# torch.stack: Creates a new dimension while stacking the tensors, the input tensors must have the same shape, returns a new tensor with one more dimension than the input tensors\n",
    "# torch.cat: concatenates tensors along an existing dimension, the tensors must have the same shape except in the dimension along which they are concatenated, does not add a new dimension.\n",
    "t1 = torch.cat([x, x, x], dim=1)\n",
    "print(t1)\n",
    "t2 = torch.cat([x, x, x], dim=0)\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.stack([x, x, x], dim=1)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36d91f",
   "metadata": {},
   "source": [
    "## Neural Network Basics\n",
    "Solve the followings tasks with the help of the [Neural networks tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html).\n",
    "\n",
    "The `nn.Module` is the basic class for layers, networks and models. All parameters of an `nn.Module` are automatically discovered by PyTorch and updated by back-propagation.\n",
    "\n",
    "First, define a neural network (as a subclass of `nn.Module`) with two linear layers and a ReLU non-linearity in between. Make the input, output, and inner dimensions parameters of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5284525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e191c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36eae143",
   "metadata": {},
   "source": [
    "Move the entire network to the GPU/MPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f976d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b22867b",
   "metadata": {},
   "source": [
    "Print the parameters of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3383e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f403132",
   "metadata": {},
   "source": [
    "Run a single forward-pass with a random input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3370725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c6d2cb7",
   "metadata": {},
   "source": [
    "Define a `nn.MSELoss` and a random target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1983de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39785fbe",
   "metadata": {},
   "source": [
    "Compute the loss and run backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5cc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e278bd02",
   "metadata": {},
   "source": [
    "Update the parameters of your network with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe16c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "927bd19f",
   "metadata": {},
   "source": [
    "Use the `AdamOptimizer` instead to update your parameters (see the [torch.optim documentation](https://pytorch.org/docs/stable/optim.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054db4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
